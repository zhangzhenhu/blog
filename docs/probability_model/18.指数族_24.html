

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>3. 指数族 &mdash; 张振虎的博客 张振虎 文档</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/graphviz.css" type="text/css" />

  
  

  
  

  
    <link rel="canonical" href="https://zhangzhenhu.github.io/blog/probability_model/18.指数族_24.html" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/translations.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"TeX": {"Macros": {"RR": "{\\bf R}", "bold": ["{\\bf #1}", 1]}}})</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="4. 多维高斯分布" href="19.%E5%A4%9A%E7%BB%B4%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83_26.html" />
    <link rel="prev" title="2. 参数估计" href="2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> 张振虎的博客
          

          
          </a>

          
            
            
              <div class="version">
                acmtiger@outlook.com
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">目录:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../glm/source/index.html">广义线性模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html">1. 概率基础</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id2">1.1. 概率模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id3">1.1.1. 概率律</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id4">1.1.2. 离散模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id5">1.1.3. 连续模型</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id6">1.2. 条件概率</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id7">1.3. 联合概率</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id8">1.4. 全概率与贝叶斯定理</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id9">1.5. 独立性</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id10">1.6. 随机变量</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id11">1.6.1. 离散随机变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id12">1.6.2. 连续随机变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id13">1.6.3. 累积分布函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id14">1.6.4. 随机变量的函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id15">1.6.5. 期望与方差</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id16">1.7. 边缘化</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id17">1.8. 常见概率分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id18">1.8.1. 伯努利分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id19">1.8.2. 二项式分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id20">1.8.3. 类别分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id21">1.8.4. 多项式分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id22">1.8.5. 高斯分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id23">1.8.6. 卡方分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#t">1.8.7. t分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#f">1.8.8. F分布</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html">2. 最大似然估计</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#ch-liklihood">2.1. 最大似然估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#id3">2.2. 伯努利分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#id4">2.3. 类别分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#ch-2-gaussian-ml">2.4. 高斯分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#id6">2.5. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html">3. 推断与检验</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id2">3.1. 统计量和充分统计量</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-sample-distribution">3.2. 抽样分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-sample-distribution-normal">3.2.1. 正态分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#t">3.2.2. t分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id5">3.2.3. 卡方分布</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id6">3.3. 极限理论</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id7">3.3.1. 马尔可夫和切比雪夫不等式</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id8">3.3.2. 弱大数定律</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id9">3.3.3. 依概率收敛</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-clt">3.3.4. 中心极限定理</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id11">3.3.5. 强大数定理</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id12">3.4. 似然估计量</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id13">3.4.1. 估计量的偏差与方差</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-2-fisher-information">3.4.2. 信息量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-2-mle-estimator">3.4.3. 最大似然估计的特性</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-influence-test-interval">3.5. 置信区间</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#z">3.5.1. 均值参数的 Z 区间估计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id18">3.5.2. 均值参数的 T 区间估计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id19">3.5.3. 方差参数的区间估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-influence-test-test">3.6. 简单假设检验</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id22">3.6.1. Z检验</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id23">3.6.2. T检验</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id24">3.6.3. 卡方检验</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html">4. 指数族</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#ch-24-1">4.1. 指数族的定义</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id3">4.1.1. 伯努利分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id4">4.1.2. 类别分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id5">4.1.3. 泊松分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id6">4.1.4. 高斯分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id7">4.1.5. 其它常见指数族</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#ch-24-moments">4.2. 指数族的期望与方差</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id9">4.3. 最大似然估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#kl">4.4. 最大似然估计与KL散度的关系</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html">5. 线性回归模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id2">5.1. 最小二乘</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id3">5.1.1. 最小误差</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id4">5.1.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id5">5.2. 线性回归的概率解释</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id6">5.2.1. 高斯分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id7">5.2.2. 参数估计</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html">6. 广义线性模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id2">6.1. 指数族分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id3">6.1.1. 自然指数族</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id4">6.1.2. 示例：高斯分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id5">6.1.3. 示例：伯努利分布</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id6">6.2. 广义线性模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id7">6.3. 例子</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html">7. 参数估计</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#ch-glm-estimate">7.1. 最大似然估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id3">7.2. 泰勒级数</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id4">7.3. 梯度下降法</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id6">7.4. 牛顿法</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id7">7.4.1. 算法推导</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id8">7.4.2. 标准连接函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id9">7.4.3. 迭代初始值的设定</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#irls">7.5. 迭代重加权最小二乘(IRLS)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id10">7.5.1. 算法推导</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id11">7.5.2. 算法过程</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id12">7.6. 估计量的标准误差</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#ch-glm-estimate-phi">7.7. 分散参数的估计</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html">8. 模型评估</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#id2">8.1. 拟合优度</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#id3">8.1.1. 嵌套模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#likelihood-ratio">8.1.2. 对数似然比(Likelihood ratio)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#deviance">8.1.3. 偏差(deviance)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#r-2">8.1.4. 决定系数 <span class="math notranslate nohighlight">\(R^2\)</span></a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#ch-glm-gof-chi">8.1.5. 广义皮尔逊卡方统计量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#residual-analysis">8.2. 残差分析(Residual analysis)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#response-residuals">8.2.1. Response residuals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#working-residuals">8.2.2. Working residuals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#partial-residuals">8.2.3. Partial residuals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#pearson-residuals">8.2.4. Pearson residuals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#deviance-residuals">8.2.5. Deviance residuals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#score-residuals">8.2.6. Score residuals</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#model-selection">8.3. 模型选择(model selection)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#aic">8.3.1. AIC</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#bic">8.3.2. BIC</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html">9. 模型检验</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id2">9.1. 拉格朗日乘子检验</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id3">9.1.1. 得分统计量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id4">9.1.2. 检验过程</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#wald">9.2. wald 检验</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id5">9.2.1. 参数估计量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id6">9.2.2. 检验过程</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id7">9.3. 似然比检验</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id8">9.3.1. 抽样分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id9">9.3.2. 模型比较</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id10">9.3.3. 偏差统计量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#f">9.3.4. F 检验</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id11">9.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html">10. 高斯模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id2">10.1. 传统线性回归</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id3">10.2. 高斯分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id4">10.3. 高斯回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id5">10.4. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id6">10.4.1. 似然函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#irls">10.4.2. IRLS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id7">10.4.3. 拟合优度</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id10">10.5. 其它连接函数</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html">11. 逆高斯模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id2">11.1. 逆高斯分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id3">11.2. 逆高斯回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id4">11.3. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id5">11.3.1. 似然函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#irls">11.3.2. IRLS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id6">11.3.3. 拟合优度</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id7">11.4. 其它连接函数</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html">12. 二项式模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id2">12.1. 伯努利分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id3">12.2. 逻辑回归模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id4">12.2.1. 模型定义</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id5">12.2.2. 参数估计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#odds-logit">12.2.3. odds 与 logit</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id6">12.3. 二项式分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id7">12.4. 二项式回归模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id8">12.4.1. 模型定义</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id9">12.4.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id10">12.5. 其它连接函数</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id11">12.5.1. 恒等连接函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#probit">12.5.2. probit 回归</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#log-log-clog-log">12.5.3. log-log 和 clog-log</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id12">12.6. 分组数据与比例数据</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html">13. 泊松模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#poisson">13.1. 泊松(Poisson)分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id2">13.1.1. 推导过程</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id3">13.1.2. 泊松分布的特性</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id5">13.2. 泊松回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id6">13.3. 参数估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id7">13.4. 拟合统计量</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id8">13.5. 频率模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id9">13.6. 泊松模型的局限性</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html">14. 指数模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#exponential">14.1. 指数(exponential)分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id2">14.1.1. 推导过程</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id3">14.1.2. 分布的特性</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id6">14.2. 指数回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id7">14.3. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id8">14.3.1. 似然函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id9">14.3.2. 拟合优度</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#irls">14.3.3. IRLS</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html">15. Gamma 模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#id1">15.1. Gamma 函数</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#id2">15.2. Gamma 分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#id3">15.3. Gamma 回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#id4">15.4. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#id5">15.4.1. 似然函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#irls">15.4.2. IRLS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#id6">15.4.3. 拟合优度</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#id7">15.5. 其他连接函数</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#id8">15.5.1. 对数 Gamma 模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#identity-gamma">15.5.2. 恒等(identity) Gamma 模型</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html">16. 过度分散</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html#id2">16.1. 什么是过度分散</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html#id3">16.2. 过度分散的检测</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html#id4">16.3. 过度分散的影响</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html#id5">16.4. 标准误差的修正</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html">17. 负二项式模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id2">17.1. 负二项式分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id3">17.1.1. 从二项式分布推导</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id4">17.1.2. 泊松-伽马混合分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#alpha">17.1.3. 辅助参数 <span class="math notranslate nohighlight">\(\alpha\)</span> 的影响</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id5">17.2. 负二项回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id6">17.3. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#irls">17.3.1. IRLS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id7">17.3.2. 参数 <span class="math notranslate nohighlight">\(\alpha\)</span> 的估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id8">17.4. 负二项式模型扩展</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id9">17.4.1. 对数连接函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id10">17.4.2. 参数 <span class="math notranslate nohighlight">\(\alpha\)</span> 的估计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id11">17.4.3. 几何模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id12">17.4.4. 广义负二项式模型</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html">18. 零计数问题</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#id2">18.1. 零截断模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#id3">18.1.1. 零截断泊松模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#id4">18.1.2. 零截断负二项式模型</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#id5">18.2. 零膨胀模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#hurdle">18.2.1. Hurdle 模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#zero-inflate">18.2.2. Zero-inflate 模型</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html">19. 多项式模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id2">19.1. 类别分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#softmax">19.2. softmax 回归模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id3">19.2.1. 模型定义</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id4">19.2.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id5">19.3. 多项式分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id6">19.4. 多项式回归模型</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html">20. 有序离散模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id2">20.1. 有序逻辑回归</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id3">20.2. 参数估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id4">20.3. 连接函数</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#logit">20.3.1. logit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#probit">20.3.2. probit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#clog-log">20.3.3. clog-log</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#log-log">20.3.4. log-log</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#cauchit">20.3.5. cauchit</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id5">20.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E9%99%84%E5%BD%95/%E5%88%86%E5%B8%83%E8%A1%A8.html">附录</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%99%84%E5%BD%95/%E5%88%86%E5%B8%83%E8%A1%A8.html#id2">标准正态累积分布表</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%99%84%E5%BD%95/%E5%88%86%E5%B8%83%E8%A1%A8.html#id3">卡方分布临界值表</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE/content.html">参考文献</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index_html.html">概率图</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html">1. 概率基础</a><ul>
<li class="toctree-l3"><a class="reference internal" href="1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id2">1.1. 概率分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id3">1.2. 独立性</a></li>
<li class="toctree-l3"><a class="reference internal" href="1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#marginalization">1.3. 边缘化(marginalization)</a></li>
<li class="toctree-l3"><a class="reference internal" href="1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id6">1.4. 贝叶斯定理</a></li>
<li class="toctree-l3"><a class="reference internal" href="1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id7">1.5. 期望与方差</a></li>
<li class="toctree-l3"><a class="reference internal" href="1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id8">1.6. 常见概率分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id9">1.6.1. 离散变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id10">1.6.2. 连续变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id11">1.6.3. 计数变量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id12">1.7. 大数定律</a><ul>
<li class="toctree-l4"><a class="reference internal" href="1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id13">1.7.1. 独立同分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id14">1.7.2. 中心极限定理</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id15">1.8. 信息论基础</a><ul>
<li class="toctree-l4"><a class="reference internal" href="1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id16">1.8.1. 信息熵</a></li>
<li class="toctree-l4"><a class="reference internal" href="1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#kl">1.8.2. KL散度</a></li>
<li class="toctree-l4"><a class="reference internal" href="1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id18">1.8.3. 互信息</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html">2. 参数估计</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#ch-liklihood">2.1. 极大似然估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id3">2.1.1. 二值离散变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id4">2.1.2. 一般离散变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#ch-2-gaussian-ml">2.1.3. 高斯分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id6">2.1.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#ch-bayesian-estimation">2.2. 贝叶斯估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id8">2.2.1. 伯努利变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id9">2.2.2. 类别变量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id10">2.3. 最大后验估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id11">2.3.1. 伯努利变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id12">2.3.2. 类别变量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id13">2.4. 最大似然估计与贝叶斯估计的对比</a></li>
<li class="toctree-l3"><a class="reference internal" href="2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id14">2.5. 统计量和充分统计量</a></li>
<li class="toctree-l3"><a class="reference internal" href="2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#fisher-information">2.6. Fisher Information</a></li>
<li class="toctree-l3"><a class="reference internal" href="2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id15">2.7. 估计量的评价</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id16">2.7.1. 估计量的方差与偏差</a></li>
<li class="toctree-l4"><a class="reference internal" href="2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id17">2.7.2. 大数定律和中心极限定理</a></li>
<li class="toctree-l4"><a class="reference internal" href="2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#ch-2-mle-estimator">2.7.3. 最大似然估计的特性</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">3. 指数族</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#ch-24-1">3.1. 指数族的定义</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id3">3.1.1. 伯努利分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id4">3.1.2. 类别分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id5">3.1.3. 泊松分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id6">3.1.4. 高斯分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id7">3.1.5. 其它常见指数族</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#ch-24-moments">3.2. 指数族的期望与方差</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id9">3.3. 最大似然估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="#kl">3.4. 最大似然估计与KL散度的关系</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="19.%E5%A4%9A%E7%BB%B4%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83_26.html">4. 多维高斯分布</a></li>
<li class="toctree-l2"><a class="reference internal" href="4.%E6%9C%89%E5%90%91%E5%9B%BE_lecture_2.html">5. 有向图(Directed Graphical Models)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="4.%E6%9C%89%E5%90%91%E5%9B%BE_lecture_2.html#id1">5.1. 有向图</a></li>
<li class="toctree-l3"><a class="reference internal" href="4.%E6%9C%89%E5%90%91%E5%9B%BE_lecture_2.html#id2">5.2. 条件独立性</a></li>
<li class="toctree-l3"><a class="reference internal" href="4.%E6%9C%89%E5%90%91%E5%9B%BE_lecture_2.html#id3">5.3. 本章总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html">6. 无向图(Undirected Graphical Models)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#id1">6.1. 无向图的定义</a></li>
<li class="toctree-l3"><a class="reference internal" href="5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#id2">6.2. 条件独立性</a></li>
<li class="toctree-l3"><a class="reference internal" href="5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#id3">6.3. 图的分解</a></li>
<li class="toctree-l3"><a class="reference internal" href="5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#vs">6.4. 有向图 vs 无向图</a></li>
<li class="toctree-l3"><a class="reference internal" href="5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#id4">6.5. 树</a></li>
<li class="toctree-l3"><a class="reference internal" href="5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#id5">6.6. 本章总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html">7. 因子图</a><ul>
<li class="toctree-l3"><a class="reference internal" href="6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#id2">7.1. 因子图的定义</a></li>
<li class="toctree-l3"><a class="reference internal" href="6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#id3">7.2. 图模型之间的转换</a><ul>
<li class="toctree-l4"><a class="reference internal" href="6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#id4">7.2.1. 转换为因子图</a></li>
<li class="toctree-l4"><a class="reference internal" href="6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#id5">7.2.2. 因子图转换为有向图</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#id6">7.3. 图模型的评价</a><ul>
<li class="toctree-l4"><a class="reference internal" href="6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#i-map">7.3.1. I-map</a></li>
<li class="toctree-l4"><a class="reference internal" href="6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#d-map">7.3.2. D-map</a></li>
<li class="toctree-l4"><a class="reference internal" href="6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#p-map">7.3.3. P-map</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="8.%E6%B6%88%E5%85%83%E6%B3%95.html">8. 模型推断：消元法</a><ul>
<li class="toctree-l3"><a class="reference internal" href="8.%E6%B6%88%E5%85%83%E6%B3%95.html#id2">8.1. 什么是模型的推断</a></li>
<li class="toctree-l3"><a class="reference internal" href="8.%E6%B6%88%E5%85%83%E6%B3%95.html#id3">8.2. 消元法</a><ul>
<li class="toctree-l4"><a class="reference internal" href="8.%E6%B6%88%E5%85%83%E6%B3%95.html#id4">8.2.1. 有向图消元算法</a></li>
<li class="toctree-l4"><a class="reference internal" href="8.%E6%B6%88%E5%85%83%E6%B3%95.html#ch-condition-margin">8.2.2. 条件概率和边缘概率</a></li>
<li class="toctree-l4"><a class="reference internal" href="8.%E6%B6%88%E5%85%83%E6%B3%95.html#id6">8.2.3. 无向图的消元法</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="8.%E6%B6%88%E5%85%83%E6%B3%95.html#id7">8.3. 图消除</a></li>
<li class="toctree-l3"><a class="reference internal" href="8.%E6%B6%88%E5%85%83%E6%B3%95.html#id9">8.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html">9. 加和乘积算法(sum-product algorithm)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id1">9.1. 树结构</a></li>
<li class="toctree-l3"><a class="reference internal" href="9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id2">9.2. 从消元法到信息传播</a></li>
<li class="toctree-l3"><a class="reference internal" href="9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id3">9.3. 树模型的和积算法</a></li>
<li class="toctree-l3"><a class="reference internal" href="9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id4">9.4. 因子图的和积算法</a></li>
<li class="toctree-l3"><a class="reference internal" href="9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id5">9.5. 类树结构图模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#polytrees">9.6. 多重树(polytrees)</a></li>
<li class="toctree-l3"><a class="reference internal" href="9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id6">9.7. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="11.%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1_lecture_11.html">10. 最大后验估计</a><ul>
<li class="toctree-l3"><a class="reference internal" href="11.%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1_lecture_11.html#id2">10.1. 最大后验概率</a></li>
<li class="toctree-l3"><a class="reference internal" href="11.%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1_lecture_11.html#id3">10.2. 最大化后验的状态</a></li>
<li class="toctree-l3"><a class="reference internal" href="11.%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1_lecture_11.html#id4">10.3. 本章总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="12.%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_20.html">11. 完整观测的参数学习</a><ul>
<li class="toctree-l3"><a class="reference internal" href="12.%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_20.html#id2">11.1. 有向图的参数学习</a></li>
<li class="toctree-l3"><a class="reference internal" href="12.%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_20.html#id3">11.2. 无向图的参数学习</a><ul>
<li class="toctree-l4"><a class="reference internal" href="12.%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_20.html#id4">11.2.1. 成对二值变量模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="12.%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_20.html#id5">11.2.2. 一般二值变量模型</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="13.%E4%B8%8D%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_22.html">12. 不完整观测的学习</a><ul>
<li class="toctree-l3"><a class="reference internal" href="13.%E4%B8%8D%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_22.html#id2">12.1. 隐变量</a></li>
<li class="toctree-l3"><a class="reference internal" href="13.%E4%B8%8D%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_22.html#em">12.2. 期望最大化算法(EM)</a></li>
<li class="toctree-l3"><a class="reference internal" href="13.%E4%B8%8D%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_22.html#id3">12.3. 隐马尔可夫模型的参数估计</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="14.%E5%9B%BE%E5%BD%A2%E7%BB%93%E6%9E%84%E7%9A%84%E5%AD%A6%E4%B9%A0_lecture_23.html">13. 有向图结构学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="16.%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD_lecture_17.html">14. 变分推断</a></li>
<li class="toctree-l2"><a class="reference internal" href="17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html">15. 马尔科夫蒙特卡洛</a><ul>
<li class="toctree-l3"><a class="reference internal" href="17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#why-sampling">15.1. Why sampling？</a></li>
<li class="toctree-l3"><a class="reference internal" href="17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#monte-carlo">15.2. 蒙特卡罗(Monte Carlo)</a></li>
<li class="toctree-l3"><a class="reference internal" href="17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#markov-chain">15.3. 马尔科夫链(Markov Chain)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#id2">15.3.1. 一个例子</a></li>
<li class="toctree-l4"><a class="reference internal" href="17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#time-reversibility">15.3.2. 时间可逆性(Time Reversibility)</a></li>
<li class="toctree-l4"><a class="reference internal" href="17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#id3">15.3.3. 总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#markov-chain-monte-carlo">15.4. Markov Chain Monte Carlo</a><ul>
<li class="toctree-l4"><a class="reference internal" href="17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#metropolis-hastings">15.4.1. Metropolis-Hastings</a></li>
<li class="toctree-l4"><a class="reference internal" href="17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#id4">15.4.2. 例子：正态分布的采样</a></li>
<li class="toctree-l4"><a class="reference internal" href="17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#id5">15.4.3. 多变量采样</a></li>
<li class="toctree-l4"><a class="reference internal" href="17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#gibbs">15.4.4. Gibbs 采样</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#mixing-time">15.5. Mixing Time</a></li>
<li class="toctree-l3"><a class="reference internal" href="17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#approximate-map-and-partitioning">15.6. Approximate MAP and Partitioning</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html">16. 贝叶斯分类器</a><ul>
<li class="toctree-l3"><a class="reference internal" href="%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id2">16.1. 朴素贝叶斯模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id3">16.1.1. 模型表示</a></li>
<li class="toctree-l4"><a class="reference internal" href="%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id4">16.1.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id5">16.2. 高斯判别模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id6">16.2.1. 一元高斯模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id7">16.2.2. 多元高斯模型</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id8">16.3. 逻辑回归</a></li>
<li class="toctree-l3"><a class="reference internal" href="%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id9">16.4. 生成模型和判别模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id10">16.5. 多分类</a></li>
<li class="toctree-l3"><a class="reference internal" href="%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id11">16.6. 其它扩展</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html">17. 回归模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id2">17.1. 机器学习的概率解释</a></li>
<li class="toctree-l3"><a class="reference internal" href="21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id3">17.2. 经典线性回归</a><ul>
<li class="toctree-l4"><a class="reference internal" href="21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id4">17.2.1. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id5">17.3. 线性回归的概率解释</a><ul>
<li class="toctree-l4"><a class="reference internal" href="21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id6">17.3.1. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id7">17.4. 凸函数最优化问题</a></li>
<li class="toctree-l3"><a class="reference internal" href="21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id8">17.5. 岭回归</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html">18. 分类模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id2">18.1. 生成模型与判别模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id3">18.2. 线性回归与线性分类</a></li>
<li class="toctree-l3"><a class="reference internal" href="22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id4">18.3. 生成模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id5">18.3.1. 高斯判别模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id6">18.3.2. 朴素贝叶斯模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id7">18.3.3. 指数族</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id8">18.4. 判别模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id9">18.4.1. 逻辑回归</a></li>
<li class="toctree-l4"><a class="reference internal" href="22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id10">18.4.2. 多分类</a></li>
<li class="toctree-l4"><a class="reference internal" href="22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id11">18.4.3. 最大熵模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#probit">18.4.4. Probit 回归</a></li>
<li class="toctree-l4"><a class="reference internal" href="22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#noisy-or">18.4.5. Noisy-OR 模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id12">18.4.6. 其它指数模型</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html">19. 广义线性模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id2">19.1. 定义</a><ul>
<li class="toctree-l4"><a class="reference internal" href="25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id3">19.1.1. 指数族分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id4">19.1.2. 链接函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id5">19.1.3. 例子</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id6">19.2. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id7">19.2.1. 梯度下降法</a></li>
<li class="toctree-l4"><a class="reference internal" href="25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id8">19.2.2. 牛顿法</a></li>
<li class="toctree-l4"><a class="reference internal" href="25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#irls">19.2.3. 迭代重加权最小二乘(IRLS)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#goodness-of-fit">19.3. goodness of fit</a></li>
<li class="toctree-l3"><a class="reference internal" href="25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id9">19.4. 连续值响应模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id10">19.4.1. 高斯族</a></li>
<li class="toctree-l4"><a class="reference internal" href="25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#gamma">19.4.2. Gamma族</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id12">19.5. 二项响应模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id13">19.6. 多项响应模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id14">19.7. 计数响应模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id15">19.7.1. 泊松分布</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id16">19.8. GLM扩展</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html">20. 混合模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#id2">20.1. 混合模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#id3">20.2. 高斯混合模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#id4">20.3. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="32.%E5%9B%A0%E5%AD%90%E5%88%86%E6%9E%90_42.html">21. 因子分析</a></li>
<li class="toctree-l2"><a class="reference internal" href="33.LDA_43.html">22. 主题模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="33.LDA_43.html#plsa">22.1. PLSA</a></li>
<li class="toctree-l3"><a class="reference internal" href="33.LDA_43.html#lda">22.2. LDA</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="26.%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB_36.html">23. 隐马尔可夫模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="26.%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB_36.html#id2">23.1. 隐马尔可夫模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="26.%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB_36.html#id3">23.1.1. 马尔可夫模型和朴素贝叶斯模型的关系</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="26.%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB_36.html#id4">23.2. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="27.%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA_37.html">24. 条件随机场</a></li>
<li class="toctree-l2"><a class="reference internal" href="28.%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2%E5%99%A8_38.html">25. 卡尔曼滤波器</a></li>
<li class="toctree-l2"><a class="reference internal" href="40.%E9%A1%B9%E7%9B%AE%E5%8F%8D%E5%BA%94%E7%90%86%E8%AE%BA_50.html">26. 项目反应理论</a></li>
<li class="toctree-l2"><a class="reference internal" href="41.%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA_51.html">27. 贝叶斯知识追踪</a></li>
<li class="toctree-l2"><a class="reference internal" href="%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE.html">28. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../audio/index.html">语音技术</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../audio/feature.html">1. 音频特征</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id2">1.1. 认识声音</a></li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id3">1.2. 认识声波</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id4">1.2.1. 物体的振动以及简谐振动</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id7">1.2.2. 什么是声波</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id8">1.2.3. 纯音和复合音</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#spectrum">1.2.4. 频谱 Spectrum</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id10">1.2.5. 名词</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id13">1.3. 语音学</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id14">1.3.1. 发声原理</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id15">1.3.2. 听觉感应</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id16">1.4. 数字信号处理</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id17">1.4.1. 模数转换</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#wav">1.4.2. 音频文件–WAV</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id18">1.5. 分帧与加窗</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id19">1.5.1. 预加重处理</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id20">1.5.2. 分帧与加窗</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id21">1.6. 声音的感官度量</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#sound-pressure-level-spl">1.6.1. 声压与声压级(Sound Pressure Level,SPL)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#intensity-level-il">1.6.2. 声强与声强级(Intensity Level,IL）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id22">1.6.3. 声压与声强的关系</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id23">1.6.4. 响度</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id24">1.6.5. 音量计算</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id25">1.6.6. 频率与音高</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id26">1.7. 时域分析</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id27">1.7.1. 短时能量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id28">1.7.2. 短时平均幅度</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id29">1.7.3. 短时过零率</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id31">1.8. 频域分析</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#spectrum-spectrogram">1.8.1. 声谱(spectrum)和时频谱(spectrogram)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#short-time-fourier-transform-stft">1.8.2. 短时傅里叶变换 Short-time Fourier transform (STFT)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id33">1.8.3. 倒频谱</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id34">1.8.4. 色谱图</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id35">1.9. 小波域特征</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id36">1.9.1. 离散小波域变换</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id37">1.9.2. 小波域过零率</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id38">1.9.3. 小波域质心</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id39">1.9.4. 小波域子带能量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#mfcc">1.10. 语音识别的音频特征–MFCC</a></li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id40">1.11. 参考资料</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../edm/index.html">教育领域数据挖掘</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../edm/bkt.html">1. 贝叶斯知识追踪(Bayesian Knowledge Tracing,BKT)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../edm/bkt.html#id1">1.1. 简介</a></li>
<li class="toctree-l3"><a class="reference internal" href="../edm/bkt.html#hidden-markov-model-hmm">1.2. 隐马尔科夫模型(Hidden Markov Model,HMM)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../edm/bkt.html#id5">1.3. 贝叶斯知识追踪(Bayesian Knowledge Tracing,BKT)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../edm/bkt.html#bkt">1.3.1. BKT的参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../edm/bkt.html#item-response-theory-irt">1.4. 项目反映理论(Item Response Theory,IRT)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../edm/bkt.html#bktirt">1.5. BKT结合IRT</a></li>
<li class="toctree-l3"><a class="reference internal" href="../edm/bkt.html#id6">1.6. 实验</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../edm/bkt.html#id7">1.6.1. 数据集</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edm/bkt.html#id8">1.6.2. 实验方法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edm/bkt.html#id9">1.6.3. 实验结果</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edm/bkt.html#id10">1.6.4. 项目代码</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../edm/bkt.html#id11">1.7. 未来工作</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../edm/bkt.html#id12">1.7.1. 题目难度的计算</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edm/bkt.html#irt">1.7.2. 多参数IRT模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edm/bkt.html#id13">1.7.3. 参数估计算法</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../edm/bkt.html#id14">1.8. 参考文献</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../nlp/index.html">自然语言处理</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html">1. 文本去重</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id2">1.1. 背景</a></li>
<li class="toctree-l3"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id3">1.2. 技术思路</a></li>
<li class="toctree-l3"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id4">1.3. 相似（距离）算法</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#euclidean-distance">1.3.1. 欧氏距离（Euclidean Distance）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#minkowski-distance">1.3.2. 闵科夫斯基距离（Minkowski Distance）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#manhattan-distance">1.3.3. 曼哈顿距离（Manhattan Distance）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#chebyshev-distance">1.3.4. 切比雪夫距离（Chebyshev Distance ）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#mahalanobis-distance">1.3.5. 马氏距离(Mahalanobis Distance)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#cosine-similarity">1.3.6. 余弦夹角相似度(Cosine Similarity)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#hamming-distance">1.3.7. 汉明距离（Hamming Distance）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#jaccard">1.3.8. Jaccard 系数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id5">1.3.9. 编辑距离</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id6">1.3.10. 最长公共字串</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id7">1.3.11. 最长公共子序列</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id8">1.4. 文本去重</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#kshingle">1.4.1. KShingle算法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#minhash">1.4.2. Minhash算法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#simhash">1.4.3. simhash</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#ksentence">1.4.4. KSentence算法</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id9">1.5. 话术去重</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../nlp/bert/content.html">2. Attention&amp;Transformer&amp;Bert 简介</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../nlp/bert/content.html#transformer">2.1. Transformer 从宏观到微观</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../nlp/bert/content.html#seq2seq">2.1.1. seq2seq</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/bert/content.html#id1">2.1.2. 模型的输入</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../nlp/bert/content.html#self-attention">2.2. Self-Attention</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../nlp/bert/content.html#id2">2.2.1. 什么是注意力？</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/bert/content.html#id3">2.2.2. 加权求和</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/bert/content.html#id4">2.2.3. 位置编码</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/bert/content.html#multi-head">2.2.4. 多头注意力（Multi-head）</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../nlp/bert/content.html#attention">2.3. Attention 机制</a></li>
<li class="toctree-l3"><a class="reference internal" href="../nlp/bert/content.html#id5">2.4. 其它参考资料</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../rst_tutorial/latex.html">latex demo</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../rst_tutorial/latex.html#latex">latex</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../rst_tutorial/latex.html#how-to-write-an-m-x-n-matrix-in-latex">How to write an m x n matrix in LaTeX</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../rst_tutorial/latex.html#with-big-parentheses">With big parentheses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../rst_tutorial/latex.html#with-parentheses">With parentheses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../rst_tutorial/latex.html#with-brackets">With brackets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../rst_tutorial/latex.html#latex-matrix-with-no-bracket">LateX matrix with no bracket</a></li>
<li class="toctree-l4"><a class="reference internal" href="../rst_tutorial/latex.html#with-vertical-bar-brackets">With vertical bar brackets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../rst_tutorial/latex.html#with-curly-brackets">with curly brackets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../rst_tutorial/latex.html#with-double-vertical-bar-brackets">with double vertical bar brackets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../rst_tutorial/latex.html#small-inline-matrix">small inline matrix</a></li>
<li class="toctree-l4"><a class="reference internal" href="../rst_tutorial/latex.html#examples-matrix-2-x-2-in-latex">Examples matrix 2 x 2 in LaTeX</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../rst_tutorial/graphviz.html">graphviz demo</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../rst_tutorial/graphviz.html#id1">布局</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rst_tutorial/graphviz.html#id2">其它</a></li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">张振虎的博客</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="index_html.html">概率图</a> &raquo;</li>
        
      <li><span class="section-number">3. </span>指数族</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/probability_model/18.指数族_24.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="id1">
<h1><span class="section-number">3. </span>指数族<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h1>
<p>本章我们介绍一类特殊概率分布，叫做指数族分布(exponential family)。
指数族分布并不是一个具体的概率分布，而是指一类分布，
这类分布具有某些共同的特性，所以它们形成了一个概率分布族(family)。
我们很多常见的概率分布都属于指数族，比如高斯分布、二项分布、多项式分布、
泊松分布、gamma分布、beta分布等等。</p>
<div class="section" id="ch-24-1">
<span id="id2"></span><h2><span class="section-number">3.1. </span>指数族的定义<a class="headerlink" href="#ch-24-1" title="永久链接至标题">¶</a></h2>
<p>一个概率分布的概率密度(质量)函数如果具有如下的形式，那么这个概率分布就属于指数族分布。</p>
<div class="math notranslate nohighlight" id="equation-eq-24-01">
<span class="eqno">(3.1.4)<a class="headerlink" href="#equation-eq-24-01" title="公式的永久链接">¶</a></span>\[p(x|\theta) = \frac{1}{Z(\theta)} h(x) \exp \{\phi(\theta)^T T(x)  \}\]</div>
<p>其中 <span class="math notranslate nohighlight">\(x\)</span> 是随机变量的取值，
<span class="math notranslate nohighlight">\(T(x),h(x),\phi(\theta)\)</span> 都是已知的函数，
通常 <span class="math notranslate nohighlight">\(h(x)\)</span> 被称为基础度量值(base measure)，
<span class="math notranslate nohighlight">\(T(x)\)</span> 是充分统计量(sufficient statistic)，
<span class="math notranslate nohighlight">\(\theta\)</span> 是分布的未知参数。
<span class="math notranslate nohighlight">\(\theta\)</span> 和 <span class="math notranslate nohighlight">\(T(x)\)</span> 可以是向量也可以是标量。
如果两个都是标量(scalar-value)， <span class="math notranslate nohighlight">\(\theta^T T(x)\)</span> 就是两者的数值乘积；如果是向量(vector-value)，
<span class="math notranslate nohighlight">\(\theta^T T(x)\)</span> 就是两者的內积。不管两者是标量还是向量， <span class="math notranslate nohighlight">\(\theta^T T(x)\)</span> 的结果都是一个实数数值。</p>
<p>函数 <span class="math notranslate nohighlight">\(Z(\theta)\)</span> 是这个分布的配分函数(partition function)，
使得这个函数是一个合法的概率密度(质量)函数， <span class="math notranslate nohighlight">\(Z(\theta)\)</span> 就是对分子的积分。</p>
<div class="math notranslate nohighlight" id="equation-eq-24-02">
<span class="eqno">(3.1.5)<a class="headerlink" href="#equation-eq-24-02" title="公式的永久链接">¶</a></span>\[Z(\theta) = \mathop{\ln} \int h(x) exp\{ \phi(\theta)^T T(x) \} dx\]</div>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>配分函数(partition function)通常出现在概率密度(质量)函数中，是为了使得这个函数的输出值符合概率约束，
即使得函数的输出值在 <span class="math notranslate nohighlight">\([0,1]\)</span> 范围内。所以，通常配分函数作为唯一的分母，其值是分子的积分。</p>
</div>
<p>通常 <a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#equation-eq-24-01">公式(4.1.1)</a> 有多种变式，比如，我们令 <span class="math notranslate nohighlight">\(g(\theta)=\frac{1}{Z(\theta)}\)</span> ，
这样可以使得式子变得更加整洁。</p>
<div class="math notranslate nohighlight" id="equation-eq-24-03">
<span class="eqno">(3.1.6)<a class="headerlink" href="#equation-eq-24-03" title="公式的永久链接">¶</a></span>\[p(x|\theta) = h(x) g(\theta) \exp \{\phi(\theta)^T T(x)  \}\]</div>
<p>有时还会把 <span class="math notranslate nohighlight">\(Z(\theta)\)</span> 移到指数的内部，
其中 <span class="math notranslate nohighlight">\(A(\theta) = \ln Z(\theta)\)</span> ，通常被称为对数配分函数(log-partition function)。</p>
<div class="math notranslate nohighlight" id="equation-eq-24-04">
<span class="eqno">(3.1.7)<a class="headerlink" href="#equation-eq-24-04" title="公式的永久链接">¶</a></span>\[ p(x|\theta) = h(x) \exp \{\phi(\theta)^T T(x) - A(\theta) \}\]</div>
<p>也有一些资料会把 <span class="math notranslate nohighlight">\(h(x)\)</span> 也移到指数内部，其中 <span class="math notranslate nohighlight">\(S(x)=\ln h(x)\)</span> 。</p>
<div class="math notranslate nohighlight" id="equation-eq-24-05">
<span class="eqno">(3.1.8)<a class="headerlink" href="#equation-eq-24-05" title="公式的永久链接">¶</a></span>\[p(x|\theta) =  \exp \{\phi(\theta)^T T(x) + S(x) - A(\theta) \}\]</div>
<p>这些不同的表示都是同一个公式的变型而已，所以它们是等价的。
为了表示方便，我们定义一个新的参数 <span class="math notranslate nohighlight">\(\eta=\phi(\theta)\)</span> ，
参数 <span class="math notranslate nohighlight">\(\eta\)</span> 通常叫做自然参数(natural parameter)或者标准参数(canonical parameter)。</p>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>“canonical parameter” 没有找到的统一的翻译，有多种翻译：标准参数、规范参数、典范参数等等。</p>
</div>
<div class="math notranslate nohighlight" id="equation-eq-24-06">
<span class="eqno">(3.1.9)<a class="headerlink" href="#equation-eq-24-06" title="公式的永久链接">¶</a></span>\[p(x|\eta) =  \exp \{ \eta^T T(x) + S(x) - A(\eta) \}\]</div>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>因为 <span class="math notranslate nohighlight">\(A(\theta)\)</span> 是对数配分函数，它是分子分积分，所以当分子中定义了 <span class="math notranslate nohighlight">\(\eta=\phi(\theta)\)</span> ，
<span class="math notranslate nohighlight">\(A(\theta)\)</span> 一定能转化成 <span class="math notranslate nohighlight">\(A(\eta)\)</span></p>
</div>
<p>在指数族中函数 <span class="math notranslate nohighlight">\(\phi(\cdot)\)</span> 总是单调连续的(存在逆函数)，所以自然参数 <span class="math notranslate nohighlight">\(\eta\)</span> 和原始参数 <span class="math notranslate nohighlight">\(\theta\)</span>
是存在一一映射关系的。
使用标准参数(canonical parameter) <span class="math notranslate nohighlight">\(\eta\)</span> 表示的公式形式称为指数族分布的标准形式(canonical form)，
在标准形式下，分布的参数是 <span class="math notranslate nohighlight">\(\eta\)</span> 。指数族中有部分分布的函数 <span class="math notranslate nohighlight">\(\phi(\cdot)\)</span> 是恒等函数，
也就是 <span class="math notranslate nohighlight">\(\eta=\phi(\theta)=\theta\)</span> ，这样的分布天然具有指数族的标准形式。
事实上，对于指数族中的任意分布，都可以通过参数转化函数 <span class="math notranslate nohighlight">\(\phi(\theta)\)</span> 把原始参数 <span class="math notranslate nohighlight">\(\theta\)</span>
转化成标准参数 <span class="math notranslate nohighlight">\(\eta\)</span> ，然后以 <span class="math notranslate nohighlight">\(\eta\)</span> 作为模型参数，进而得到标准形式(canonical form)。
下面我们列举一些属于指数族分布的例子。</p>
<div class="section" id="id3">
<h3><span class="section-number">3.1.1. </span>伯努利分布<a class="headerlink" href="#id3" title="永久链接至标题">¶</a></h3>
<p>伯努利分布的概率质量函数为：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-18-24-0">
<span class="eqno">(3.1.10)<a class="headerlink" href="#equation-probability-model-18-24-0" title="公式的永久链接">¶</a></span>\[p(x|\mu) = \mu^x(1-\mu)^{1-x}\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\mu\)</span> 表示这个概率分布的参数，
我们可以把右侧改写一下：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-18-24-1">
<span class="eqno">(3.1.11)<a class="headerlink" href="#equation-probability-model-18-24-1" title="公式的永久链接">¶</a></span>\[ \begin{align}\begin{aligned}p(x|\mu) &amp;= \mu^x(1-\mu)^{1-x}\\&amp;= exp \{  \ln  [ \mu^x (1-\mu)^{1-x}  ] \}\\&amp;= exp\{  x \ln \mu + (1-x) \ln (1-\mu)  \}\\&amp;= exp\{  x \ln \left( \frac{\mu}{1-\mu} \right) +  \ln (1- \mu)  \}\end{aligned}\end{align} \]</div>
<p>和 <a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#equation-eq-24-06">公式(4.1.7)</a> 对比下，可以发现有：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-18-24-2">
<span class="eqno">(3.1.12)<a class="headerlink" href="#equation-probability-model-18-24-2" title="公式的永久链接">¶</a></span>\[ \begin{align}\begin{aligned}\eta &amp;=\phi(\mu) = \ln \left( \frac{\mu}{1-\mu} \right)\\T(x) &amp;=x\\A(\eta) &amp;= - ln (1-\mu) = ln(1+e^{\eta})\\S(x) &amp;= 0\end{aligned}\end{align} \]</div>
<p>函数 <span class="math notranslate nohighlight">\(\ln \left( \frac{\mu}{1-\mu} \right)\)</span> 被称为logit函数：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-18-24-3">
<span class="eqno">(3.1.13)<a class="headerlink" href="#equation-probability-model-18-24-3" title="公式的永久链接">¶</a></span>\[\eta=\phi(\mu)=logit(\mu)=\ln \left( \frac{\mu}{1-\mu} \right)\]</div>
<p>logit函数的反函数是sigmoid函数。</p>
<div class="math notranslate nohighlight" id="equation-probability-model-18-24-4">
<span class="eqno">(3.1.14)<a class="headerlink" href="#equation-probability-model-18-24-4" title="公式的永久链接">¶</a></span>\[\mu = sigmoid(\eta)=\frac{e^{\eta}}{1+e^{\eta}} = \frac{1}{1+e^{-\eta}}\]</div>
<p>sigmoid函数又被称为逻辑函数(logistic function)，在以后的章节中还会再遇到它。</p>
</div>
<div class="section" id="id4">
<h3><span class="section-number">3.1.2. </span>类别分布<a class="headerlink" href="#id4" title="永久链接至标题">¶</a></h3>
<p>伯努利分布是只有两个取值的离散随机变量的概率分布，当随机变量的取值扩展到多个(大于2个并且有限集)的时候，就是称为类别分布，
也可以认为是单一观测(一个样本，一次实验)的多项式分布。
其概率质量函数为：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-18-24-5">
<span class="eqno">(3.1.15)<a class="headerlink" href="#equation-probability-model-18-24-5" title="公式的永久链接">¶</a></span>\[p(x|\theta) = \prod_{k=1}^m \theta_k^{x_k}\]</div>
<p>其中m表示变量有m种取值，注意 <span class="math notranslate nohighlight">\(x_k \in \{0,1\}\)</span> ，表示变量是否为第k个值，
当变量值是第k个值时 <span class="math notranslate nohighlight">\(x_k=1\)</span> ，否则为0。
<span class="math notranslate nohighlight">\(\theta_k\)</span> 表示 <span class="math notranslate nohighlight">\(x_k=1\)</span> 的概率，并且有 <span class="math notranslate nohighlight">\(\sum_{k=1}^m \theta_k=1\)</span> 。</p>
<p>同样我们需要把上式变型成指数族形式。</p>
<div class="math notranslate nohighlight" id="equation-probability-model-18-24-6">
<span class="eqno">(3.1.16)<a class="headerlink" href="#equation-probability-model-18-24-6" title="公式的永久链接">¶</a></span>\[p(x|\theta) = \prod_{k=1}^m \theta_k^{x_k}= \exp \{ \sum_{k=1}^m x_k \ln \theta_k \}\]</div>
<p>然而我们注意到，其中m个参数 <span class="math notranslate nohighlight">\(\theta_k\)</span> 是冗余的，因为有 <span class="math notranslate nohighlight">\(\sum_{k=1}^m \theta_k=1\)</span> ，
其中 <span class="math notranslate nohighlight">\(\theta_m\)</span> 可以用 <span class="math notranslate nohighlight">\(\theta_m=1-\sum_{k=1}^{m-1} \theta_k\)</span> 表示，
模型只需要m-1个参数，而不需要m个参数。</p>
<div class="math notranslate nohighlight" id="equation-probability-model-18-24-7">
<span class="eqno">(3.1.17)<a class="headerlink" href="#equation-probability-model-18-24-7" title="公式的永久链接">¶</a></span>\[ \begin{align}\begin{aligned}p(x|\theta) &amp;=  \exp \{ \sum_{k=1}^m x_k \ln \theta_k \}\\&amp;=  \exp \left \{ \sum_{k=1}^{m-1} x_k \ln \theta_k  +
\left (1-\sum_{k=1}^{m-1} x_k \right ) \ln \left (1-\sum_{k=1}^{m-1} \theta_k \right )   \right \}\\&amp;= \exp \left \{ \sum_{k=1}^{m-1} x_k \ln \left ( \frac{\theta_k}{1-\sum_{j=1}^{m-1} \theta_j} \right )
+ \ln \left  (1-\sum_{k=1}^{m-1} \theta_k \right ) \right \}\\&amp;= \exp \left \{ \sum_{k=1}^{m-1} x_k \ln \left ( \frac{\theta_k}{ \theta_m} \right )
+ \ln \left  (1-\sum_{k=1}^{m-1} \theta_k \right ) \right \}\\
&amp;= \exp \left \{ \phi(\theta)^T T(x) - A(\theta) \right \}\end{aligned}\end{align} \]</div>
<p>上式中的 <span class="math notranslate nohighlight">\(\sum_{k=1}^{m-1} x_k \ln \left ( \frac{\theta_k}{ \theta_m} \right )\)</span>
可以看做是向量 <span class="math notranslate nohighlight">\(\phi(\theta) = [\phi(\theta_1),\dots,\phi(\theta_k),\dots,\phi(\theta_{m-1})]\)</span>
和向量 <span class="math notranslate nohighlight">\(T(x)=[x_1,\dots,x_k,\dots,x_{m-1}]\)</span> 的內积。
和 <a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#equation-eq-24-06">公式(4.1.7)</a> 对比下，可以发现有：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-18-24-8">
<span class="eqno">(3.1.18)<a class="headerlink" href="#equation-probability-model-18-24-8" title="公式的永久链接">¶</a></span>\[ \begin{align}\begin{aligned}\eta &amp;= \phi(\theta) = [\phi(\theta_1),\dots,\phi(\theta_k),\dots,\phi(\theta_{m-1})]
,\phi(\theta_k) = \ln \left ( \frac{\theta_k}{ \theta_m} \right )\\T(x)&amp;=[x_1,\dots,x_k,\dots,x_{m-1}]\\ A(\eta) &amp;= - \ln \left  (1-\sum_{k=1}^{m-1} \theta_k \right )
= \ln \left (  \sum_{k=1}^m e^{\eta_k}  \right )\\S(x) &amp;= 0\end{aligned}\end{align} \]</div>
<p>用 <span class="math notranslate nohighlight">\(\eta\)</span> 表示 <span class="math notranslate nohighlight">\(\theta\)</span> 有：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-18-24-9">
<span class="eqno">(3.1.19)<a class="headerlink" href="#equation-probability-model-18-24-9" title="公式的永久链接">¶</a></span>\[\theta_k = \frac{e^{\eta_k}}{\sum_{j=1}^m e^{\eta_j}}\]</div>
<p>这个函数被称为softmax函数。</p>
</div>
<div class="section" id="id5">
<h3><span class="section-number">3.1.3. </span>泊松分布<a class="headerlink" href="#id5" title="永久链接至标题">¶</a></h3>
<p>泊松(Poisson)分布的概率质量函数为：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-18-24-10">
<span class="eqno">(3.1.20)<a class="headerlink" href="#equation-probability-model-18-24-10" title="公式的永久链接">¶</a></span>\[p(x|\theta) = \frac{\theta^x e^{-\theta}}{x!}\]</div>
<p>我们同样对它进行改写：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-18-24-11">
<span class="eqno">(3.1.21)<a class="headerlink" href="#equation-probability-model-18-24-11" title="公式的永久链接">¶</a></span>\[ \begin{align}\begin{aligned}p(x|\theta) &amp;= \frac{ \exp\{ \ln [ \theta^x e^{-\theta} ]  \}}{x!}\\&amp;= \frac{ \exp\{ x \ln  \theta   -\theta  \} }{x!}\\&amp;= \exp \{ x \ln  \theta   -\theta  -  \ln x! \}\end{aligned}\end{align} \]</div>
<p>和 <a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#equation-eq-24-06">公式(4.1.7)</a> 对比可得：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-18-24-12">
<span class="eqno">(3.1.22)<a class="headerlink" href="#equation-probability-model-18-24-12" title="公式的永久链接">¶</a></span>\[ \begin{align}\begin{aligned}\eta &amp;= \phi(\theta) = \ln \theta\\T(x) &amp;= x\\A(\eta) &amp;=  \theta = e^{\eta}\\S(x) &amp;= - \ln x!\end{aligned}\end{align} \]</div>
<p><span class="math notranslate nohighlight">\(\eta\)</span> 和 <span class="math notranslate nohighlight">\(\theta\)</span> 的关系为：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-18-24-13">
<span class="eqno">(3.1.23)<a class="headerlink" href="#equation-probability-model-18-24-13" title="公式的永久链接">¶</a></span>\[\theta = e^{\eta}\]</div>
</div>
<div class="section" id="id6">
<h3><span class="section-number">3.1.4. </span>高斯分布<a class="headerlink" href="#id6" title="永久链接至标题">¶</a></h3>
<p>这里我们只考虑单维高斯模型，高斯模型有两个参数，分别是均值参数 <span class="math notranslate nohighlight">\(\mu\)</span>
和方差参数 <span class="math notranslate nohighlight">\(\sigma^2\)</span> ，高斯分布的概率密度函数为：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-18-24-14">
<span class="eqno">(3.1.24)<a class="headerlink" href="#equation-probability-model-18-24-14" title="公式的永久链接">¶</a></span>\[p(x|\mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} exp \left \{ -\frac{1}{2\sigma^2}(x-\mu)^2 \right \}\]</div>
<p>我们将其转化成指数族的标准形式。</p>
<div class="math notranslate nohighlight" id="equation-probability-model-18-24-15">
<span class="eqno">(3.1.25)<a class="headerlink" href="#equation-probability-model-18-24-15" title="公式的永久链接">¶</a></span>\[ \begin{align}\begin{aligned}p(x|\mu,\sigma^2) &amp;= \frac{1}{\sqrt{2\pi\sigma^2}} exp \left \{ -\frac{1}{2\sigma^2}(x-\mu)^2 \right \}\\&amp; =  exp \left \{ -\frac{1}{2\sigma^2} x^2 + \frac{\mu}{\sigma^2} x -\frac{1}{2\sigma^2}\mu^2
 -\ln \sigma - \frac{1}{2} \ln (2\pi) \right \}\end{aligned}\end{align} \]</div>
<p>和 <a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#equation-eq-24-06">公式(4.1.7)</a> 对比可得：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-18-24-16">
<span class="eqno">(3.1.26)<a class="headerlink" href="#equation-probability-model-18-24-16" title="公式的永久链接">¶</a></span>\[ \begin{align}\begin{aligned}\eta &amp;= \phi(\theta)=\left [ \mu / \sigma^2 ,-1 /2 \sigma^2 \right ]\\T(x) &amp;= \left [ x ,x^2 \right ]\\
A(\eta) &amp;= \frac{\mu^2}{2\sigma^2} +ln \sigma = -\frac{\eta_1^2}{4\eta_2} - \frac{1}{2}ln(-2\eta_2)\\S(x) &amp;= - \frac{1}{2} \ln (2\pi)\end{aligned}\end{align} \]</div>
<p>注意单变量高斯模型是含有两个参数的，所以 <span class="math notranslate nohighlight">\(\eta\)</span> 和 <span class="math notranslate nohighlight">\(T(x)\)</span> 都是一个长度为2的向量。
多维高斯模型同样也属于指数族，可以自己推导下。</p>
</div>
<div class="section" id="id7">
<h3><span class="section-number">3.1.5. </span>其它常见指数族<a class="headerlink" href="#id7" title="永久链接至标题">¶</a></h3>
<p>请参考：<a class="reference external" href="https://en.wikipedia.org/wiki/Exponential_family#Table_of_distributions">维基百科 https://en.wikipedia.org/wiki/Exponential_family#Table_of_distributions</a></p>
</div>
</div>
<div class="section" id="ch-24-moments">
<span id="id8"></span><h2><span class="section-number">3.2. </span>指数族的期望与方差<a class="headerlink" href="#ch-24-moments" title="永久链接至标题">¶</a></h2>
<blockquote>
<div><p>在数学和统计学中，矩（moment）是对变量分布和形态特点的一组度量。
n阶矩被定义为一变量的n次方与其概率密度函数（Probability Density Function, PDF）之积的积分。
在文献中n阶矩通常用符号 <span class="math notranslate nohighlight">\(\mu_n\)</span> 表示，直接使用变量计算的矩被称为原始矩（raw moment），
移除均值后计算的矩被称为中心矩（central moment）。
变量的一阶原始矩等价于数学期望（expectation）、二至四阶中心矩被定义为方差（variance）、偏度（skewness）和峰度（kurtosis）。</p>
<p class="attribution">—摘自百度百科</p>
</div></blockquote>
<p>通俗的讲，矩（moment）是描述一个随机变量的一系列指标，变量的期望(Expectation，或者叫均值，Mean)和方差(Variance)属于其中最简单的两个指标，我们这里只讨论这两种。</p>
<p>指数族有一个特点，就是我们可以通过对 <span class="math notranslate nohighlight">\(A(\eta)\)</span> 求导来得到 <span class="math notranslate nohighlight">\(T(x)\)</span> 的矩，
比如其一阶导数是 <span class="math notranslate nohighlight">\(T(x)\)</span> 的期望，二阶导数是 <span class="math notranslate nohighlight">\(T(x)\)</span> 的方差。
在指数族分布中 <span class="math notranslate nohighlight">\(A(\eta) = \mathop{\ln} \int h(x) exp\{ \eta^T T(x) \} dx\)</span>
，其一阶导数为：</p>
<div class="math notranslate nohighlight" id="equation-eq-24-10">
<span class="eqno">(3.2.22)<a class="headerlink" href="#equation-eq-24-10" title="公式的永久链接">¶</a></span>\[ \begin{align}\begin{aligned}\frac{d A}{d \eta} &amp;= \frac{d}{d \eta} \left \{
\mathop{\ln}  \int h(x) exp\{ \eta^T T(x) \} dx   \right \}\\&amp;= \frac{  \int T(x) exp\{ \eta^T T(x) \}h(x)dx    }
{    \int  exp\{ \eta^T T(x) \}h(x)dx   }\\&amp;= \int T(x) exp \{ \eta^T T(x) - A(\eta)    \} h(x) dx\\&amp;= \mathbb{E}[T(x)]\end{aligned}\end{align} \]</div>
<p>我们看到 <span class="math notranslate nohighlight">\(A(\eta)\)</span> 的一阶导数正好等于 <span class="math notranslate nohighlight">\(T(x)\)</span> 的期望(均值)，对于伯努利分布、多项分布、泊松分布、高斯分布等这些
<span class="math notranslate nohighlight">\(T(x)=x\)</span> 的分布来说，<span class="math notranslate nohighlight">\(T(x)\)</span> 的均值就是分布的均值。</p>
<p>比如上面的示例中，对于伯努利分布，有 <span class="math notranslate nohighlight">\(A(\eta)=ln(1+e^\eta)\)</span> ，
其一阶导数为：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-18-24-17">
<span class="eqno">(3.2.23)<a class="headerlink" href="#equation-probability-model-18-24-17" title="公式的永久链接">¶</a></span>\[ \begin{align}\begin{aligned}\frac{d A}{d \eta} &amp;= \frac{d }{d \eta} ln(1+e^{\eta})\\&amp;= \frac{e^{\eta}}{1 + e^{\eta}}\\&amp;= \frac{1}{1+e^{-\eta}}\\&amp;= \mu\end{aligned}\end{align} \]</div>
<p>对于高斯分布有：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-18-24-18">
<span class="eqno">(3.2.24)<a class="headerlink" href="#equation-probability-model-18-24-18" title="公式的永久链接">¶</a></span>\[A(\eta) = -\frac{\eta_1^2}{4\eta_2} - \frac{1}{2}ln(-2\eta_2)\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\eta_1=\mu/\sigma^2,\eta_2=-1/2\sigma^2\)</span> ，我们计算 <span class="math notranslate nohighlight">\(\eta_1\)</span> 的偏导数：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-18-24-19">
<span class="eqno">(3.2.25)<a class="headerlink" href="#equation-probability-model-18-24-19" title="公式的永久链接">¶</a></span>\[ \begin{align}\begin{aligned}\frac{\partial A}{\partial \eta_1} &amp;= \frac{\eta_1}{2\eta_2}\\&amp;= \frac{\mu/\sigma^2}{1/\sigma^2}\\&amp;= \mu\end{aligned}\end{align} \]</div>
<p>现在我们看下 <span class="math notranslate nohighlight">\(A(\eta)\)</span> 的二阶导数：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-18-24-20">
<span class="eqno">(3.2.26)<a class="headerlink" href="#equation-probability-model-18-24-20" title="公式的永久链接">¶</a></span>\[ \begin{align}\begin{aligned}\frac{d^2 A}{d\eta^2} &amp;=
\int T(x) exp \{ \eta T(x) -A(\eta)\}\ (T(x)- A'(\eta)) h(x) dx\\&amp;= \int T(x) exp \{\eta T(x) - A(\eta)\}(T(x)-\mathbb{E}[ T(x)] ) h(x) dx\\&amp;= \int T(x)^2 exp\{ \eta T(x)-A(\eta) \} h(x)dx -
\mathbb{E}[ T(x)] \int T(x) exp\{\eta T(x) -A(\eta)\} h(x) dx\\&amp;= \mathbb{E}[ T(x)^2] - (\mathbb{E}[ T(x)])^2\\&amp;= Var [ T(x)]\end{aligned}\end{align} \]</div>
<p><span class="math notranslate nohighlight">\(A(\eta)\)</span> 的二阶导数正好是 <span class="math notranslate nohighlight">\(T(x)\)</span> 的方差，对于 <span class="math notranslate nohighlight">\(T(x)=x\)</span> 的分布，就是分布的方差。</p>
<p>比如对于高斯分布，对于 <span class="math notranslate nohighlight">\(\eta_1\)</span> 的二阶偏导数为：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-18-24-21">
<span class="eqno">(3.2.27)<a class="headerlink" href="#equation-probability-model-18-24-21" title="公式的永久链接">¶</a></span>\[ \begin{align}\begin{aligned}\frac{\partial A}{\partial \eta_1} &amp;=- \frac{1}{2\eta_2}\\&amp;= \sigma^2\end{aligned}\end{align} \]</div>
<p>总结一下，对于指数族分布， 我们可以通过对 <span class="math notranslate nohighlight">\(A{\eta}\)</span> 求导来计算分布中 <span class="math notranslate nohighlight">\(T(x)\)</span> 期望和方差，
当然通过高阶导数还能计算出更多的矩(Moment)。</p>
<p>此外，我们发现函数 <span class="math notranslate nohighlight">\(A{\eta}\)</span> 的二阶导数是 <span class="math notranslate nohighlight">\(T(x)\)</span> 的方差，我们都知道方差肯定是大于等于0的，
一个函数的二阶导数大于等于0，证明这个函数是一个凸函数(convex，碗状的)，
对于凸函数，一阶导数和参数 <span class="math notranslate nohighlight">\(\eta\)</span> 之间是一一对应关系，并且这种对应关系是可逆的。
我们定义 <span class="math notranslate nohighlight">\(A(\eta)\)</span> 的一阶导数用符号 <span class="math notranslate nohighlight">\(\mu\)</span> 表示，则有 <span class="math notranslate nohighlight">\(u\triangleq \mathbb{E}[T(x)]\)</span>
， <span class="math notranslate nohighlight">\(\mu\)</span> 和 <span class="math notranslate nohighlight">\(\eta\)</span> 之间的关系可以用如下函数表示：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-18-24-22">
<span class="eqno">(3.2.28)<a class="headerlink" href="#equation-probability-model-18-24-22" title="公式的永久链接">¶</a></span>\[\mu = \frac{d A}{d \eta}\]</div>
<p>并且这个函数是可逆的，也就是说已知 <span class="math notranslate nohighlight">\(\mu\)</span> 就能求出 <span class="math notranslate nohighlight">\(\eta\)</span> ；
反过来，已知 <span class="math notranslate nohighlight">\(\eta\)</span> 就能求出 <span class="math notranslate nohighlight">\(\mu\)</span> 。
比如对于伯努利分布：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-18-24-23">
<span class="eqno">(3.2.29)<a class="headerlink" href="#equation-probability-model-18-24-23" title="公式的永久链接">¶</a></span>\[ \begin{align}\begin{aligned}\eta &amp;= \frac{\mu}{1-\mu}\\\mu &amp;= \frac{1}{1+e^{-\eta}} \ \text{(logistic function)}\end{aligned}\end{align} \]</div>
<p>对于多项式分布：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-18-24-24">
<span class="eqno">(3.2.30)<a class="headerlink" href="#equation-probability-model-18-24-24" title="公式的永久链接">¶</a></span>\[ \begin{align}\begin{aligned}\eta_i &amp;= \ln \left ( \frac{\mu_i}{1-\sum_{i=1}^{m-1}} \right )\\\mu_i &amp;= \frac{e^{\eta_i}}{\sum_{j=1}^{m} e^{\eta_j}} \ \text{(softmax function)}\end{aligned}\end{align} \]</div>
<p>由于 <span class="math notranslate nohighlight">\(\mu\)</span> 和 <span class="math notranslate nohighlight">\(\eta\)</span> 是可逆的，所以对于指数族分布，也可以用 <span class="math notranslate nohighlight">\(\mu\)</span> 去定义分布模型，也就是用
<span class="math notranslate nohighlight">\(\mu\)</span> 去当做模型的参数。事实上，我们常见的分布都是这么做的，比如伯努利分布、高斯分布等等。</p>
</div>
<div class="section" id="id9">
<h2><span class="section-number">3.3. </span>最大似然估计<a class="headerlink" href="#id9" title="永久链接至标题">¶</a></h2>
<p>现在我们讨论下指数族的最大似然估计，
我们知道指数族的自然参数 <span class="math notranslate nohighlight">\(\eta\)</span> 和特定分布的原始参数 <span class="math notranslate nohighlight">\(\theta\)</span> 是一一对应的，二者是存在可逆关系的，
所有只要我们能估计出自然参数 <span class="math notranslate nohighlight">\(\eta\)</span> ，就一定能通过逆函数 <span class="math notranslate nohighlight">\(\phi(\cdot)^{-1}\)</span> 得到分布的真实参数
<span class="math notranslate nohighlight">\(\theta\)</span> 的估计值，也就是说对于指数族，我们只需要推导自然参数的估计量 <span class="math notranslate nohighlight">\(\hat{eta}\)</span>  即可。</p>
<p>我们用符号 <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> 表示随机变量的一个观测样本集，样本集的规模是N，
并且样本集是满足IID(独立同分布)的。</p>
<p>首先回顾一下指数族分布的标准形式：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-18-24-25">
<span class="eqno">(3.3.28)<a class="headerlink" href="#equation-probability-model-18-24-25" title="公式的永久链接">¶</a></span>\[p(x|\eta) =  exp \{\eta^T T(x) - A(\eta) + S(x) \}\]</div>
<p>我们知道样本的似然就是所有样本发生的联合概率：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-18-24-26">
<span class="eqno">(3.3.29)<a class="headerlink" href="#equation-probability-model-18-24-26" title="公式的永久链接">¶</a></span>\[ \begin{align}\begin{aligned}L(\eta;\mathcal{D}) &amp;= p(\mathcal{D}|\eta)\\&amp;= p(x_1,\dots,x_N|\eta)\\&amp;= \prod_{i=1}^N  p(x_i|\eta)\\&amp;= \prod_{i=1}^N  \exp \{\eta^T T(x_i) - A(\eta) + S(x_i) \}\\&amp;=  \exp \{ \eta^T \sum_{i=1}^N  T(x_i) - N A(\eta) + \sum_{i=1}^N S(x_i) \}\end{aligned}\end{align} \]</div>
<p>对比一下，我们发现指数族分布的联合概率仍然是指数族：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-18-24-27">
<span class="eqno">(3.3.30)<a class="headerlink" href="#equation-probability-model-18-24-27" title="公式的永久链接">¶</a></span>\[ \begin{align}\begin{aligned}T(x) &amp;\Longrightarrow \sum_{i=1}^N  T(x_i)\\A(\eta) &amp;\Longrightarrow N A(\eta)\\S(x) &amp;\Longrightarrow \sum_{i=1}^N S(x_i)\end{aligned}\end{align} \]</div>
<p>现在我们为似然函数加上对数，得到对数似然函数：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-18-24-28">
<span class="eqno">(3.3.31)<a class="headerlink" href="#equation-probability-model-18-24-28" title="公式的永久链接">¶</a></span>\[ \begin{align}\begin{aligned}\ell(\eta;\mathcal{D}) &amp;= \ln L(\eta;\mathcal{D})\\&amp;=  \eta^T \sum_{i=1}^N  T(x_i) - N A(\eta) + \sum_{i=1}^N S(x_i)\end{aligned}\end{align} \]</div>
<p>我们对参数 <span class="math notranslate nohighlight">\(\eta\)</span> 求导：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-18-24-29">
<span class="eqno">(3.3.32)<a class="headerlink" href="#equation-probability-model-18-24-29" title="公式的永久链接">¶</a></span>\[\nabla_{\eta} \ell = \sum_{i=1}^{N} T(x_i) - N \nabla_{\eta} A(\eta)\]</div>
<p>上述公式中的 <span class="math notranslate nohighlight">\(\nabla_{\eta} A(\eta)\)</span> 表示对函数 <span class="math notranslate nohighlight">\(A(\eta)\)</span> 关于 <span class="math notranslate nohighlight">\(\eta\)</span> 求导，
这里函数 <span class="math notranslate nohighlight">\(A(\eta)\)</span> 是一个关于 <span class="math notranslate nohighlight">\(\eta\)</span> 的函数。
我们令这个导数为0，可得：</p>
<div class="math notranslate nohighlight" id="equation-eq-24-18">
<span class="eqno">(3.3.33)<a class="headerlink" href="#equation-eq-24-18" title="公式的永久链接">¶</a></span>\[\nabla_{\eta} A(\eta) = \frac{1}{N} \sum_{i=1}^{N} T(x_i)\]</div>
<p>由 <a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#equation-eq-24-10">公式(4.2.1)</a> 我们知道 <span class="math notranslate nohighlight">\(A(\eta)\)</span> 的一阶导数等于 <span class="math notranslate nohighlight">\(T(x)\)</span> 的期望 <span class="math notranslate nohighlight">\(\mathbb{E}[T(x)]\)</span> ，
即 <span class="math notranslate nohighlight">\(\nabla_{\eta} A(\eta)=\mathbb{E}[T(x)]\)</span> 。
我们令 <span class="math notranslate nohighlight">\(\mu \triangleq \nabla_{\eta} A(\eta) =\mathbb{E}[T(x)]\)</span> ，
结合公式 <a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#equation-eq-24-18">公式(4.3.6)</a> 有：</p>
<div class="math notranslate nohighlight" id="equation-eq-24-19">
<span class="eqno">(3.3.34)<a class="headerlink" href="#equation-eq-24-19" title="公式的永久链接">¶</a></span>\[\mu_{ML}=\mathbb{E}[T(x)] =  \frac{1}{N} \sum_{i=1}^{N} T(x)\]</div>
<p>从 <a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#equation-eq-24-19">公式(4.3.7)</a> 可以看出，指数族分布理论期望值(均值参数)等于样本的期望值(平均值)。
均值参数的最大似然估计值，只和样本的统计量 <span class="math notranslate nohighlight">\(\sum_{i=1}^N T(x)\)</span> 有关，
而不再依赖样本的其它信息，所以 <span class="math notranslate nohighlight">\(\sum_{i=1}^N T(x)\)</span> (或者说 <span class="math notranslate nohighlight">\(T(x)\)</span> )是指数族的充分统计量。
对于满足 <span class="math notranslate nohighlight">\(T(x)=x\)</span> 的分布，比如伯努利分布、多项式分布、泊松分布等等，样本的均值就是 <span class="math notranslate nohighlight">\(T(x)\)</span> 的均值，
<strong>样本的均值就是均值参数的最大似然估计值</strong> 。
同理，对于单变量的高斯分布，样本的方差就是方差参数的最大似然估计值。</p>
<p>我们知道 <span class="math notranslate nohighlight">\(\mu\)</span> 和 <span class="math notranslate nohighlight">\(\eta\)</span> 是一一对应的，可以通过一个函数进行互相计算，最大似然估计给出了
<span class="math notranslate nohighlight">\(\mu_{ML}\)</span> 的估计值，我们就是可以换算出 <span class="math notranslate nohighlight">\(\eta_{ML}\)</span> 。
前文说过，事实上对于很多常见分布是直接用 <span class="math notranslate nohighlight">\(\mu\)</span> 作为参数的，所以有了最大似然的估计值 <span class="math notranslate nohighlight">\(\mu_{ML}\)</span>
就直接是模型的参数估计值。
<a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#equation-eq-24-19">公式(4.3.7)</a> 也直接说明了当样本数量趋近无穷大时，最大似然估计值和 <span class="math notranslate nohighlight">\(\mu\)</span> 的真实值是一致的。</p>
</div>
<div class="section" id="kl">
<h2><span class="section-number">3.4. </span>最大似然估计与KL散度的关系<a class="headerlink" href="#kl" title="永久链接至标题">¶</a></h2>
<p>本节我们讨论一下指数族的最大似然估计和KL散度的关系，在开始前我们先回顾一下KL散度的定义。</p>
<dl class="glossary simple">
<dt id="term-KL-KullbackLeibler-divergence">KL散度（Kullback–Leibler divergence)<a class="headerlink" href="#term-KL-KullbackLeibler-divergence" title="Permalink to this term">¶</a></dt><dd><p>KL散度（Kullback–Leibler divergence，简称KLD），在信息系统中称为相对熵（relative entropy），
在连续时间序列中称为randomness，在统计模型推断中称为信息增益（information gain），
也称信息散度（information divergence）。
KL散度是两个概率分布P和Q差别的 <strong>非对称性</strong> 的度量，可以理解成是用来度量两个分布的相似性。
一般用符号 <span class="math notranslate nohighlight">\(D_{KL}(P \parallel Q)\)</span> 表示。</p>
</dd>
</dl>
<p>对于离散随机变量，概率分布P和Q的KL散度按照下式定义：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-18-24-30">
<span class="eqno">(3.4.35)<a class="headerlink" href="#equation-probability-model-18-24-30" title="公式的永久链接">¶</a></span>\[D_{KL}(P \parallel Q) = \sum_{i} P(i) \ln \frac{P(i)}{Q(i)}\]</div>
<p>或者：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-18-24-31">
<span class="eqno">(3.4.36)<a class="headerlink" href="#equation-probability-model-18-24-31" title="公式的永久链接">¶</a></span>\[D_{KL}(P \parallel Q) = -\sum_{i} P(i) \ln \frac{Q(i)}{P(i)}\]</div>
<p>即按照概率P求得P和Q的对数商的平均值(期望)，其中对数的底可以是任意的。
KL散度仅当概率P和Q各自总和均为1，且对于任何i皆满足
<span class="math notranslate nohighlight">\(Q(i)&gt;0,P(i)&gt;0\)</span> 时才有定义。
式中出现 <span class="math notranslate nohighlight">\(0\ln 0\)</span> 的情况，其值按0处理。</p>
<p>对于连续随机变量，其概率分布P和Q可按积分方式定义为:</p>
<div class="math notranslate nohighlight" id="equation-probability-model-18-24-32">
<span class="eqno">(3.4.37)<a class="headerlink" href="#equation-probability-model-18-24-32" title="公式的永久链接">¶</a></span>\[D_{KL}(P \parallel Q) = \int P(x) \ln \frac{P(x)}{Q(x)} dx\]</div>
<p>相对熵的值为非负数 <span class="math notranslate nohighlight">\(D_{KL}(P \parallel Q) \ge 0\)</span> ，
由吉布斯不等式可知，当且仅当P = Q时 <span class="math notranslate nohighlight">\(D_{KL}(P \parallel Q)\)</span> 为零。
尽管从直觉上KL散度是个度量或距离函数, 但是它实际上并不是一个真正的度量或距离。
因为KL散度不具有对称性：从分布P到Q的距离通常并不等于从Q到P的距离。</p>
<div class="math notranslate nohighlight" id="equation-probability-model-18-24-33">
<span class="eqno">(3.4.38)<a class="headerlink" href="#equation-probability-model-18-24-33" title="公式的永久链接">¶</a></span>\[D_{KL}(P \parallel Q) \neq D_{KL}(Q \parallel P)\]</div>
<p>我们可以根据信息理论量重写对数似然函数，其中
<span class="math notranslate nohighlight">\(x_m\)</span> 为随机变量的一个可能取值，
<span class="math notranslate nohighlight">\(\hat{p}_{\mathcal{D}}(x_m)\)</span> 表示在样本中变量值为 <span class="math notranslate nohighlight">\(x_m\)</span>
的样本出现的比例，乘以N后就是出现的次数。
我们用 <span class="math notranslate nohighlight">\(\hat{p}_{\mathcal{D}}(x_m)\)</span> 表示从样本中的到的经验分布。
此外，定义 <span class="math notranslate nohighlight">\(n_m\)</span> 表示样本中  <span class="math notranslate nohighlight">\(x_m\)</span> 出现的次数，则有
<span class="math notranslate nohighlight">\(n_m=N \hat{p}_{\mathcal{D}}(x_m)\)</span> 。</p>
<div class="math notranslate nohighlight" id="equation-probability-model-18-24-34">
<span class="eqno">(3.4.39)<a class="headerlink" href="#equation-probability-model-18-24-34" title="公式的永久链接">¶</a></span>\[ \begin{align}\begin{aligned}\ell(\eta;\mathcal{D}) &amp;= \sum_{i=1}^{N} \log p(x^{(i)};\eta)\\&amp;= \sum_{x_m \in \mathcal{X} } \log  p(x_m;\eta)^{n_{m}}\\&amp;= N \sum_{x_m \in \mathcal{X} } \hat{p}_{\mathcal{D}}(x_m) \log p(x_m;\eta)\\&amp;= N \sum_{x_m \in \mathcal{X} } \hat{p}_{\mathcal{D}}(x_m) [ \log p(x_m;\eta) -log \hat{p}_{\mathcal{D}}(x_m) + log  \hat{p}_{\mathcal{D}}(x_m) ]\\&amp;= N \sum_{x_m \in \mathcal{X} } \hat{p}_{\mathcal{D}}(x_m) [ \log \frac{p(x_m;\eta)}{\hat{p}_{\mathcal{D}}(x_m)} + log  \hat{p}_{\mathcal{D}}(x_m) ]\\&amp;= N \underbrace{\sum_{x_m \in \mathcal{X} } \hat{p}_{\mathcal{D}}(x_m)
\log \frac{p(x_m;\eta)}{\hat{p}_{\mathcal{D}}(x_m)}}_{\text{负的KL散度}}
+ N \underbrace{ \sum_{x_m \in \mathcal{X} } \hat{p}_{\mathcal{D}}(x_m) log  \hat{p}_{\mathcal{D}}(x_m)}_{\text{经验分布的信息熵}}\\
&amp;= N( H(\hat{p}_{\mathcal{D}}) - D( \hat{p}_{\mathcal{D}} \parallel p(x ; \eta)  ) )\end{aligned}\end{align} \]</div>
<p>我们可以忽略熵项，因为它是经验分布的函数，与参数 <span class="math notranslate nohighlight">\(\eta\)</span> 无关，在极大化过程中其值是固定值。
因此，<strong>最大化似然等同于最小化经验分布与真实分布的信息差异</strong>
<span class="math notranslate nohighlight">\(D( \hat{p}_{\mathcal{D}} \parallel p(\cdot ; \eta))\)</span> 。</p>
<p>回想一下，当两个分布是相同的分布时，KL散度为零。 在多项式情况下，
由于我们在有限空间 <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> 上优化了所有分布的集合，我们可以精确地匹配分布，例如，
令 <span class="math notranslate nohighlight">\(p(\cdot;\eta)=\hat{p}_{\mathcal{D}}\)</span> ，即可使KL散度为零，得到精确匹配。
然而，在大多数有趣的问题中，我们无法完全匹配数据分布(如果可以，我们只会过度拟合）。
相反，我们通常优化由 <span class="math notranslate nohighlight">\(\eta\)</span> 参数化的受限类分布，来得到近似解。</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="19.%E5%A4%9A%E7%BB%B4%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83_26.html" class="btn btn-neutral float-right" title="4. 多维高斯分布" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html" class="btn btn-neutral float-left" title="2. 参数估计" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>


    <script src="https://utteranc.es/client.js"
            repo="zhangzhenhu/blog"
            issue-term="pathname"
            theme="github-light"
            crossorigin="anonymous"
            async>
    </script>



  <div role="contentinfo">
    <p>
        &#169; 版权所有 2018, zhangzhenhu(acmtiger@outlook.com) 禁止一切形式的转载！.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>.



</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>
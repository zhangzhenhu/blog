

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>推荐算法之从FM到DeepFM &mdash; 张振虎的博客 张振虎 文档</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />

  
  

  
  

  
    <link rel="canonical" href="https://zhangzhenhu.github.io/blog/data_mining/ctr.html" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/translations.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"TeX": {"Macros": {"RR": "{\\bf R}", "bold": ["{\\bf #1}", 1]}}})</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> 张振虎的博客
          

          
          </a>

          
            
            
              <div class="version">
                acmtiger@outlook.com
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">目录:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../glm/source/index.html">广义线性模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html">1. 概率基础</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id2">1.1. 概率模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id3">1.1.1. 概率律</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id4">1.1.2. 离散模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id5">1.1.3. 连续模型</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id6">1.2. 条件概率</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id7">1.3. 联合概率</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id8">1.4. 全概率与贝叶斯定理</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id9">1.5. 独立性</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id10">1.6. 随机变量</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id11">1.6.1. 离散随机变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id12">1.6.2. 连续随机变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id13">1.6.3. 累积分布函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id14">1.6.4. 随机变量的函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id15">1.6.5. 期望与方差</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id16">1.7. 边缘化</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id17">1.8. 常见概率分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id18">1.8.1. 伯努利分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id19">1.8.2. 二项式分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id20">1.8.3. 类别分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id21">1.8.4. 多项式分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id22">1.8.5. 高斯分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id23">1.8.6. 卡方分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#t">1.8.7. t分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#f">1.8.8. F分布</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html">2. 最大似然估计</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#ch-liklihood">2.1. 最大似然估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#id3">2.2. 伯努利分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#id4">2.3. 类别分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#ch-2-gaussian-ml">2.4. 高斯分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#id6">2.5. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html">3. 推断与检验</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id2">3.1. 统计量和充分统计量</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-sample-distribution">3.2. 抽样分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-sample-distribution-normal">3.2.1. 正态分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#t">3.2.2. t分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id5">3.2.3. 卡方分布</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id6">3.3. 极限理论</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id7">3.3.1. 马尔可夫和切比雪夫不等式</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id8">3.3.2. 弱大数定律</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id9">3.3.3. 依概率收敛</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-clt">3.3.4. 中心极限定理</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id11">3.3.5. 强大数定理</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id12">3.4. 似然估计量</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id13">3.4.1. 估计量的偏差与方差</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-2-fisher-information">3.4.2. 信息量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-2-mle-estimator">3.4.3. 最大似然估计的特性</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-influence-test-interval">3.5. 置信区间</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#z">3.5.1. 均值参数的 Z 区间估计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id18">3.5.2. 均值参数的 T 区间估计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id19">3.5.3. 方差参数的区间估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-influence-test-test">3.6. 简单假设检验</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id22">3.6.1. Z检验</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id23">3.6.2. T检验</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id24">3.6.3. 卡方检验</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html">4. 指数族</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#ch-24-1">4.1. 指数族的定义</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id3">4.1.1. 伯努利分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id4">4.1.2. 类别分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id5">4.1.3. 泊松分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id6">4.1.4. 高斯分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id7">4.1.5. 其它常见指数族</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#ch-24-moments">4.2. 指数族的期望与方差</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id9">4.3. 最大似然估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#kl">4.4. 最大似然估计与KL散度的关系</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html">5. 线性回归模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id2">5.1. 最小二乘</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id3">5.1.1. 最小误差</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id4">5.1.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id5">5.2. 线性回归的概率解释</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id6">5.2.1. 高斯分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id7">5.2.2. 参数估计</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html">6. 广义线性模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id2">6.1. 指数族分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id3">6.1.1. 自然指数族</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id4">6.1.2. 示例：高斯分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id5">6.1.3. 示例：伯努利分布</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id6">6.2. 广义线性模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id7">6.3. 例子</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html">7. 参数估计</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#ch-glm-estimate">7.1. 最大似然估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id3">7.2. 泰勒级数</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id4">7.3. 梯度下降法</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id6">7.4. 牛顿法</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id7">7.4.1. 算法推导</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id8">7.4.2. 标准连接函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id9">7.4.3. 迭代初始值的设定</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#irls">7.5. 迭代重加权最小二乘(IRLS)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id10">7.5.1. 算法推导</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id11">7.5.2. 算法过程</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id12">7.6. 估计量的标准误差</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#ch-glm-estimate-phi">7.7. 分散参数的估计</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html">8. 模型评估</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#id2">8.1. 拟合优度</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#id3">8.1.1. 嵌套模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#likelihood-ratio">8.1.2. 对数似然比(Likelihood ratio)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#deviance">8.1.3. 偏差(deviance)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#r-2">8.1.4. 决定系数 <span class="math notranslate nohighlight">\(R^2\)</span></a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#ch-glm-gof-chi">8.1.5. 广义皮尔逊卡方统计量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#residual-analysis">8.2. 残差分析(Residual analysis)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#response-residuals">8.2.1. Response residuals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#working-residuals">8.2.2. Working residuals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#partial-residuals">8.2.3. Partial residuals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#pearson-residuals">8.2.4. Pearson residuals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#deviance-residuals">8.2.5. Deviance residuals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#score-residuals">8.2.6. Score residuals</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#model-selection">8.3. 模型选择(model selection)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#aic">8.3.1. AIC</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#bic">8.3.2. BIC</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html">9. 模型检验</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id2">9.1. 拉格朗日乘子检验</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id3">9.1.1. 得分统计量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id4">9.1.2. 检验过程</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#wald">9.2. wald 检验</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id5">9.2.1. 参数估计量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id6">9.2.2. 检验过程</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id7">9.3. 似然比检验</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id8">9.3.1. 抽样分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id9">9.3.2. 模型比较</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id10">9.3.3. 偏差统计量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#f">9.3.4. F 检验</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id11">9.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html">10. 高斯模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id2">10.1. 传统线性回归</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id3">10.2. 高斯分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id4">10.3. 高斯回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id5">10.4. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id6">10.4.1. 似然函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#irls">10.4.2. IRLS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id7">10.4.3. 拟合优度</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id10">10.5. 其它连接函数</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html">11. 逆高斯模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id2">11.1. 逆高斯分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id3">11.2. 逆高斯回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id4">11.3. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id5">11.3.1. 似然函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#irls">11.3.2. IRLS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id6">11.3.3. 拟合优度</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id7">11.4. 其它连接函数</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html">12. 二项式模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id2">12.1. 伯努利分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id3">12.2. 逻辑回归模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id4">12.2.1. 模型定义</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id5">12.2.2. 参数估计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#odds-logit">12.2.3. odds 与 logit</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id6">12.3. 二项式分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id7">12.4. 二项式回归模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id8">12.4.1. 模型定义</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id9">12.4.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id10">12.5. 其它连接函数</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id11">12.5.1. 恒等连接函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#probit">12.5.2. probit 回归</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#log-log-clog-log">12.5.3. log-log 和 clog-log</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id12">12.6. 分组数据与比例数据</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html">13. 泊松模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#poisson">13.1. 泊松(Poisson)分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id2">13.1.1. 推导过程</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id3">13.1.2. 泊松分布的特性</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id5">13.2. 泊松回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id6">13.3. 参数估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id7">13.4. 拟合统计量</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id8">13.5. 频率模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id9">13.6. 泊松模型的局限性</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html">14. 指数模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#exponential">14.1. 指数(exponential)分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id2">14.1.1. 推导过程</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id3">14.1.2. 分布的特性</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id6">14.2. 指数回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id7">14.3. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id8">14.3.1. 似然函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id9">14.3.2. 拟合优度</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#irls">14.3.3. IRLS</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html">15. Gamma 模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#id1">15.1. Gamma 函数</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#id2">15.2. Gamma 分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#id3">15.3. Gamma 回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#id4">15.4. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#id5">15.4.1. 似然函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#irls">15.4.2. IRLS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#id6">15.4.3. 拟合优度</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#id7">15.5. 其他连接函数</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#id8">15.5.1. 对数 Gamma 模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#identity-gamma">15.5.2. 恒等(identity) Gamma 模型</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html">16. 过度分散</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html#id2">16.1. 什么是过度分散</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html#id3">16.2. 过度分散的检测</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html#id4">16.3. 过度分散的影响</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html#id5">16.4. 标准误差的修正</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html">17. 负二项式模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id2">17.1. 负二项式分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id3">17.1.1. 从二项式分布推导</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id4">17.1.2. 泊松-伽马混合分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#alpha">17.1.3. 辅助参数 <span class="math notranslate nohighlight">\(\alpha\)</span> 的影响</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id5">17.2. 负二项回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id6">17.3. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#irls">17.3.1. IRLS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id7">17.3.2. 参数 <span class="math notranslate nohighlight">\(\alpha\)</span> 的估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id8">17.4. 负二项式模型扩展</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id9">17.4.1. 对数连接函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id10">17.4.2. 参数 <span class="math notranslate nohighlight">\(\alpha\)</span> 的估计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id11">17.4.3. 几何模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id12">17.4.4. 广义负二项式模型</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html">18. 零计数问题</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#id2">18.1. 零截断模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#id3">18.1.1. 零截断泊松模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#id4">18.1.2. 零截断负二项式模型</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#id5">18.2. 零膨胀模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#hurdle">18.2.1. Hurdle 模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#zero-inflate">18.2.2. Zero-inflate 模型</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html">19. 多项式模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id2">19.1. 类别分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#softmax">19.2. softmax 回归模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id3">19.2.1. 模型定义</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id4">19.2.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id5">19.3. 多项式分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id6">19.4. 多项式回归模型</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html">20. 有序离散模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id2">20.1. 有序逻辑回归</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id3">20.2. 参数估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id4">20.3. 连接函数</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#logit">20.3.1. logit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#probit">20.3.2. probit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#clog-log">20.3.3. clog-log</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#log-log">20.3.4. log-log</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#cauchit">20.3.5. cauchit</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id5">20.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E9%99%84%E5%BD%95/%E5%88%86%E5%B8%83%E8%A1%A8.html">附录</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%99%84%E5%BD%95/%E5%88%86%E5%B8%83%E8%A1%A8.html#id2">标准正态累积分布表</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%99%84%E5%BD%95/%E5%88%86%E5%B8%83%E8%A1%A8.html#id3">卡方分布临界值表</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE/content.html">参考文献</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../probability_model/index_html.html">概率图</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html">1. 概率基础</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id2">1.1. 概率分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id3">1.2. 独立性</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#marginalization">1.3. 边缘化(marginalization)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id6">1.4. 贝叶斯定理</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id7">1.5. 期望与方差</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id8">1.6. 常见概率分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id9">1.6.1. 离散变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id10">1.6.2. 连续变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id11">1.6.3. 计数变量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id12">1.7. 大数定律</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id13">1.7.1. 独立同分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id14">1.7.2. 中心极限定理</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id15">1.8. 信息论基础</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id16">1.8.1. 信息熵</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#kl">1.8.2. KL散度</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id18">1.8.3. 互信息</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html">2. 参数估计</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#ch-liklihood">2.1. 极大似然估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id3">2.1.1. 二值离散变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id4">2.1.2. 一般离散变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#ch-2-gaussian-ml">2.1.3. 高斯分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id6">2.1.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#ch-bayesian-estimation">2.2. 贝叶斯估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id8">2.2.1. 伯努利变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id9">2.2.2. 类别变量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id10">2.3. 最大后验估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id11">2.3.1. 伯努利变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id12">2.3.2. 类别变量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id13">2.4. 最大似然估计与贝叶斯估计的对比</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id14">2.5. 统计量和充分统计量</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#fisher-information">2.6. Fisher Information</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id15">2.7. 估计量的评价</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id16">2.7.1. 估计量的方差与偏差</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id17">2.7.2. 大数定律和中心极限定理</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#ch-2-mle-estimator">2.7.3. 最大似然估计的特性</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html">3. 指数族</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#ch-24-1">3.1. 指数族的定义</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#id3">3.1.1. 伯努利分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#id4">3.1.2. 类别分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#id5">3.1.3. 泊松分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#id6">3.1.4. 高斯分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#id7">3.1.5. 其它常见指数族</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#ch-24-moments">3.2. 指数族的期望与方差</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#id9">3.3. 最大似然估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#kl">3.4. 最大似然估计与KL散度的关系</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/19.%E5%A4%9A%E7%BB%B4%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83_26.html">4. 多维高斯分布</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/4.%E6%9C%89%E5%90%91%E5%9B%BE_lecture_2.html">5. 有向图(Directed Graphical Models)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/4.%E6%9C%89%E5%90%91%E5%9B%BE_lecture_2.html#id1">5.1. 有向图</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/4.%E6%9C%89%E5%90%91%E5%9B%BE_lecture_2.html#id2">5.2. 条件独立性</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/4.%E6%9C%89%E5%90%91%E5%9B%BE_lecture_2.html#id3">5.3. 本章总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html">6. 无向图(Undirected Graphical Models)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#id1">6.1. 无向图的定义</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#id2">6.2. 条件独立性</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#id3">6.3. 图的分解</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#vs">6.4. 有向图 vs 无向图</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#id4">6.5. 树</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#id5">6.6. 本章总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html">7. 因子图</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#id2">7.1. 因子图的定义</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#id3">7.2. 图模型之间的转换</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#id4">7.2.1. 转换为因子图</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#id5">7.2.2. 因子图转换为有向图</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#id6">7.3. 图模型的评价</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#i-map">7.3.1. I-map</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#d-map">7.3.2. D-map</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#p-map">7.3.3. P-map</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95_lecture_7.html">8. 消元法(The Elimination Algorithm)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95_lecture_7.html#id1">8.1. 什么是模型的推断</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95_lecture_7.html#id2">8.2. 消元法</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95_lecture_7.html#id3">8.2.1. 一个例子</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95_lecture_7.html#ch-condition-margin">8.2.2. 条件概率和边缘概率</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95_lecture_7.html#id5">8.2.3. 有向图的消元法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95_lecture_7.html#id6">8.2.4. 无向图的消元法</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95_lecture_7.html#id7">8.3. 图消除</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95_lecture_7.html#id9">8.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html">9. 加和乘积算法(sum-product algorithm)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id1">9.1. 树结构</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id2">9.2. 从消元法到信息传播</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id3">9.3. 树模型的和积算法</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id4">9.4. 因子图的和积算法</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id5">9.5. 类树结构图模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#polytrees">9.6. 多重树(polytrees)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id6">9.7. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/11.%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1_lecture_11.html">10. 最大后验估计</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/11.%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1_lecture_11.html#id2">10.1. 最大后验概率</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/11.%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1_lecture_11.html#id3">10.2. 最大化后验的状态</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/11.%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1_lecture_11.html#id4">10.3. 本章总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/12.%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_20.html">11. 完整观测的参数学习</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/12.%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_20.html#id2">11.1. 有向图的参数学习</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/12.%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_20.html#id3">11.2. 无向图的参数学习</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/12.%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_20.html#id4">11.2.1. 成对二值变量模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/12.%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_20.html#id5">11.2.2. 一般二值变量模型</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/13.%E4%B8%8D%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_22.html">12. 不完整观测的学习</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/13.%E4%B8%8D%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_22.html#id2">12.1. 隐变量</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/13.%E4%B8%8D%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_22.html#em">12.2. 期望最大化算法(EM)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/13.%E4%B8%8D%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_22.html#id3">12.3. 隐马尔可夫模型的参数估计</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/14.%E5%9B%BE%E5%BD%A2%E7%BB%93%E6%9E%84%E7%9A%84%E5%AD%A6%E4%B9%A0_lecture_23.html">13. 有向图结构学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/16.%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD_lecture_17.html">14. 变分推断</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html">15. 马尔科夫蒙特卡洛</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#why-sampling">15.1. Why sampling？</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#monte-carlo">15.2. 蒙特卡罗(Monte Carlo)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#markov-chain">15.3. 马尔科夫链(Markov Chain)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#id2">15.3.1. 一个例子</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#time-reversibility">15.3.2. 时间可逆性(Time Reversibility)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#id3">15.3.3. 总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#markov-chain-monte-carlo">15.4. Markov Chain Monte Carlo</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#metropolis-hastings">15.4.1. Metropolis-Hastings</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#id4">15.4.2. 例子：正态分布的采样</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#id5">15.4.3. 多变量采样</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#gibbs">15.4.4. Gibbs 采样</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#mixing-time">15.5. Mixing Time</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#approximate-map-and-partitioning">15.6. Approximate MAP and Partitioning</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html">16. 回归模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id2">16.1. 机器学习的概率解释</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id3">16.2. 经典线性回归</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id4">16.2.1. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id5">16.3. 线性回归的概率解释</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id6">16.3.1. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id7">16.4. 凸函数最优化问题</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id8">16.5. 岭回归</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html">17. 分类模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id2">17.1. 生成模型与判别模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id3">17.2. 线性回归与线性分类</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id4">17.3. 生成模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id5">17.3.1. 高斯判别模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id6">17.3.2. 朴素贝叶斯模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id7">17.3.3. 指数族</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id8">17.4. 判别模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id9">17.4.1. 逻辑回归</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id10">17.4.2. 多分类</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id11">17.4.3. 最大熵模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#probit">17.4.4. Probit 回归</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#noisy-or">17.4.5. Noisy-OR 模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id12">17.4.6. 其它指数模型</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html">18. 广义线性模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id2">18.1. 定义</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id3">18.1.1. 指数族分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id4">18.1.2. 链接函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id5">18.1.3. 例子</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id6">18.2. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id7">18.2.1. 梯度下降法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id8">18.2.2. 牛顿法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#irls">18.2.3. 迭代重加权最小二乘(IRLS)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#goodness-of-fit">18.3. goodness of fit</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id9">18.4. 连续值响应模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id10">18.4.1. 高斯族</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#gamma">18.4.2. Gamma族</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id12">18.5. 二项响应模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id13">18.6. 多项响应模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id14">18.7. 计数响应模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id15">18.7.1. 泊松分布</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id16">18.8. GLM扩展</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html">19. 混合模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#id2">19.1. 混合模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#id3">19.2. 高斯混合模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#id4">19.3. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/32.%E5%9B%A0%E5%AD%90%E5%88%86%E6%9E%90_42.html">20. 因子分析</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/33.LDA_43.html">21. 主题模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/33.LDA_43.html#plsa">21.1. PLSA</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/33.LDA_43.html#lda">21.2. LDA</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/26.%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB_36.html">22. 隐马尔可夫模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/26.%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB_36.html#id2">22.1. 隐马尔可夫模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/26.%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB_36.html#id3">22.1.1. 马尔可夫模型和朴素贝叶斯模型的关系</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/26.%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB_36.html#id4">22.2. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/27.%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA_37.html">23. 条件随机场</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/28.%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2%E5%99%A8_38.html">24. 卡尔曼滤波器</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/40.%E9%A1%B9%E7%9B%AE%E5%8F%8D%E5%BA%94%E7%90%86%E8%AE%BA_50.html">25. 项目反应理论</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/41.%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA_51.html">26. 贝叶斯知识追踪</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE.html">27. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../audio/index.html">语音技术</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../audio/feature.html">1. 音频特征</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id2">1.1. 认识声音</a></li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id3">1.2. 认识声波</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id4">1.2.1. 物体的振动以及简谐振动</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id7">1.2.2. 什么是声波</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id8">1.2.3. 纯音和复合音</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#spectrum">1.2.4. 频谱 Spectrum</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id10">1.2.5. 名词</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id13">1.3. 语音学</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id14">1.3.1. 发声原理</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id15">1.3.2. 听觉感应</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id16">1.4. 数字信号处理</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id17">1.4.1. 模数转换</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#wav">1.4.2. 音频文件–WAV</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id18">1.5. 分帧与加窗</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id19">1.5.1. 预加重处理</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id20">1.5.2. 分帧与加窗</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id21">1.6. 声音的感官度量</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#sound-pressure-level-spl">1.6.1. 声压与声压级(Sound Pressure Level,SPL)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#intensity-level-il">1.6.2. 声强与声强级(Intensity Level,IL）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id22">1.6.3. 声压与声强的关系</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id23">1.6.4. 响度</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id24">1.6.5. 音量计算</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id25">1.6.6. 频率与音高</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id26">1.7. 时域分析</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id27">1.7.1. 短时能量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id28">1.7.2. 短时平均幅度</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id29">1.7.3. 短时过零率</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id31">1.8. 频域分析</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#spectrum-spectrogram">1.8.1. 声谱(spectrum)和时频谱(spectrogram)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#short-time-fourier-transform-stft">1.8.2. 短时傅里叶变换 Short-time Fourier transform (STFT)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id33">1.8.3. 倒频谱</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id34">1.8.4. 色谱图</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id35">1.9. 小波域特征</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id36">1.9.1. 离散小波域变换</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id37">1.9.2. 小波域过零率</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id38">1.9.3. 小波域质心</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id39">1.9.4. 小波域子带能量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#mfcc">1.10. 语音识别的音频特征–MFCC</a></li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id40">1.11. 参考资料</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../edm/index.html">教育领域数据挖掘</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../edm/bkt.html">1. 贝叶斯知识追踪(Bayesian Knowledge Tracing,BKT)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../edm/bkt.html#id1">1.1. 简介</a></li>
<li class="toctree-l3"><a class="reference internal" href="../edm/bkt.html#hidden-markov-model-hmm">1.2. 隐马尔科夫模型(Hidden Markov Model,HMM)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../edm/bkt.html#id5">1.3. 贝叶斯知识追踪(Bayesian Knowledge Tracing,BKT)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../edm/bkt.html#bkt">1.3.1. BKT的参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../edm/bkt.html#item-response-theory-irt">1.4. 项目反映理论(Item Response Theory,IRT)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../edm/bkt.html#bktirt">1.5. BKT结合IRT</a></li>
<li class="toctree-l3"><a class="reference internal" href="../edm/bkt.html#id6">1.6. 实验</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../edm/bkt.html#id7">1.6.1. 数据集</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edm/bkt.html#id8">1.6.2. 实验方法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edm/bkt.html#id9">1.6.3. 实验结果</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edm/bkt.html#id10">1.6.4. 项目代码</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../edm/bkt.html#id11">1.7. 未来工作</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../edm/bkt.html#id12">1.7.1. 题目难度的计算</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edm/bkt.html#irt">1.7.2. 多参数IRT模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edm/bkt.html#id13">1.7.3. 参数估计算法</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../edm/bkt.html#id14">1.8. 参考文献</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">张振虎的博客</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>推荐算法之从FM到DeepFM</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/data_mining/ctr.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="fmdeepfm">
<h1>推荐算法之从FM到DeepFM<a class="headerlink" href="#fmdeepfm" title="永久链接至标题">¶</a></h1>
<p>推荐与ctr</p>
<div class="section" id="latent-factor">
<h2>一、潜在因子（Latent Factor）算法<a class="headerlink" href="#latent-factor" title="永久链接至标题">¶</a></h2>
<p>这种算法是在NetFlix（没错，就是用大数据捧火《纸牌屋》的那家公司）的推荐算法竞赛中获奖的算法，最早被应用于电影推荐中。
下面仅利用基础的矩阵知识来介绍下这种算法。</p>
<p>这种算法的思想是这样：每个用户（user）都有自己的偏好，比如A喜欢带有小清新的、吉他伴奏的、王菲等元素（latent factor），
如果一首歌（item）带有这些元素，那么就将这首歌推荐给该用户，也就是用元素去连接用户和音乐。每个人对不同的元素偏好不同，
而每首歌包含的元素也不一样。我们希望能找到这样两个矩阵：</p>
<blockquote>
<div><p>一.用户-潜在因子矩阵 <span class="math notranslate nohighlight">\(Q\)</span> ，</p>
</div></blockquote>
<p>表示不同的用户对于不用元素的偏好程度，1代表很喜欢，0代表不喜欢。比如这样：</p>
<blockquote>
<div><img alt="../_images/mf1.jpg" src="../_images/mf1.jpg" />
<p>二.潜在因子-音乐矩阵 <span class="math notranslate nohighlight">\(P\)</span></p>
</div></blockquote>
<p>表示每种音乐含有各种元素的成分，比如下表中，音乐A是一个偏小清新的音乐，含有小清新这个Latent Factor的成分是0.9，
重口味的成分是0.1，优雅的成分是0.2……</p>
<blockquote>
<div><img alt="../_images/mf2.jpg" src="../_images/mf2.jpg" />
</div></blockquote>
<dl>
<dt>利用这两个矩阵，我们能得出张三对音乐A的喜欢程度是：</dt><dd><p><strong>张三对小清新的偏好*音乐A含有小清新的成分+对重口味的偏好*音乐A含有重口味的成分+对优雅的偏好*音乐A含有优雅的成分+……</strong></p>
<img alt="../_images/mf3.jpg" src="../_images/mf3.jpg" />
<img alt="../_images/mf4.jpg" src="../_images/mf4.jpg" />
<p>即：0.6*0.9+0.8*0.1+0.1*0.2+0.1*0.4+0.7*0=0.69</p>
</dd>
</dl>
<p>每个用户对每首歌都这样计算可以得到不同用户对不同歌曲的评分矩阵 <span class="math notranslate nohighlight">\(\tilde{R}\)</span>  。（注，这里的破浪线表示的是估计的评分，不带波浪线的 <span class="math notranslate nohighlight">\(R\)</span> 表示实际的评分）：</p>
<blockquote>
<div><img alt="../_images/mf5.jpg" src="../_images/mf5.jpg" />
</div></blockquote>
<p>因此我们对张三推荐四首歌中得分最高的B，对李四推荐得分最高的C，王五推荐B。</p>
<p>如果用矩阵表示即为：</p>
<div class="math notranslate nohighlight" id="equation-data-mining-ctr-0">
<span class="eqno">()<a class="headerlink" href="#equation-data-mining-ctr-0" title="公式的永久链接">¶</a></span>\[\tilde{R} =QP^{T}\]</div>
<p>下面问题来了，这个潜在因子（latent factor）是怎么得到的呢？</p>
<p>由于面对海量的让用户自己给音乐分类并告诉我们自己的偏好系数显然是不现实的，<strong>事实上我们能获得的数据只有用户行为数据</strong>。</p>
<p>首先进行量化：单曲循环=5, 分享=4, 收藏=3, 主动播放=2 , 听完=1, 跳过=-2 , 拉黑=-5，在分析时能获得的实际评分矩阵 <span class="math notranslate nohighlight">\(R\)</span> ，也就是输入矩阵大概是这个样子：</p>
<blockquote>
<div><img alt="../_images/mf6.jpg" src="../_images/mf6.jpg" />
</div></blockquote>
<p><strong>事实上这是个非常非常稀疏的矩阵</strong> ，因为大部分用户只听过全部音乐中很少一部分。如何利用这个矩阵去找潜在因子呢？这里主要应用到的是矩阵的UV分解。
也就是将上面的评分矩阵分解为两个低维度的矩阵，用Q和P两个矩阵的乘积去估计实际的评分矩阵</p>
<blockquote>
<div><img alt="../_images/mf7.jpg" src="../_images/mf7.jpg" />
</div></blockquote>
<p>使得评分矩阵 <span class="math notranslate nohighlight">\(\tilde{R}\)</span>  与 实际的评分矩阵越接近越好，也就是求解如下的目标函数：</p>
<blockquote>
<div><img alt="../_images/mf8.png" src="../_images/mf8.png" />
</div></blockquote>
<p>利用梯度下降法就可以求得这 <span class="math notranslate nohighlight">\(P\)</span> , <span class="math notranslate nohighlight">\(Q\)</span> 两个矩阵的估计值
例如我们上面给出的那个例子可以分解成为这样两个矩阵：</p>
<blockquote>
<div><img alt="../_images/mf9.jpg" src="../_images/mf9.jpg" />
</div></blockquote>
<p>这两个矩阵相乘就可以得到估计的得分矩阵：</p>
<blockquote>
<div><img alt="../_images/mf10.jpg" src="../_images/mf10.jpg" />
</div></blockquote>
</div>
<div class="section" id="fm-factorization-machine">
<h2>FM（Factorization Machine）<a class="headerlink" href="#fm-factorization-machine" title="永久链接至标题">¶</a></h2>
<p>由Konstanz大学Steffen Rendle（现任职于Google）于2010年最早提出的，<strong>旨在解决稀疏数据下的特征组合问题</strong></p>
<p>下面以一个示例引入FM模型。假设一个广告分类的问题，根据用户和广告位相关的特征，预测用户是否点击了广告。</p>
<blockquote>
<div><img alt="../_images/fm1.png" src="../_images/fm1.png" />
</div></blockquote>
<p>“Clicked?“是label，Country、Day、Ad_type是特征。由于三种特征都是categorical类型的，需要经过独热编码（One-Hot Encoding）转换成数值型特征。</p>
<blockquote>
<div><img alt="../_images/fm2.png" src="../_images/fm2.png" />
</div></blockquote>
<p>由上表可以看出，经过One-Hot编码之后，大部分样本数据特征是比较稀疏的。上面的样例中，每个样本有7维特征，但平均仅有3维特征具有非零值。
实际上，这种情况并不是此例独有的，在真实应用场景中这种情况普遍存在。例如，CTR/CVR预测时，用户的性别、职业、教育水平、品类偏好，
商品的品类等，经过One-Hot编码转换后都会导致样本数据的稀疏性。特别是商品品类这种类型的特征，如商品的末级品类约有550个，
采用One-Hot编码生成550个数值特征，但每个样本的这550个特征，有且仅有一个是有效的（非零）。由此可见，<strong>数据稀疏性是实际问题中不可避免的挑战</strong>。</p>
<p><strong>One-Hot编码的另一个特点就是导致特征空间大</strong>。例如，商品品类有550维特征，一个categorical特征转换为550维数值特征，特征空间剧增。</p>
<p>同时通过观察大量的样本数据可以发现，<strong>某些特征经过关联之后，与label之间的相关性就会提高</strong> 。例如，“USA”与“Thanksgiving”、“China”与“Chinese New Year”这样的关联特征，
对用户的点击有着正向的影响。换句话说，来自“China”的用户很可能会在“Chinese New Year”有大量的浏览、购买行为，而在“Thanksgiving”却不会有特别的消费行为。
这种关联特征与label的正向相关性在实际问题中是普遍存在的，如“化妆品”类商品与“女”性，“球类运动配件”的商品与“男”性，“电影票”的商品与“电影”品类偏好等。
因此，引入两个特征的组合是非常有意义的。</p>
<p><a href="#id1"><span class="problematic" id="id2">**</span></a>多项式模型**是包含特征组合的最直观的模型。在多项式模型中，特征 <span class="math notranslate nohighlight">\(x_i\)</span> 和 <span class="math notranslate nohighlight">\(x_j\)</span> 的组合采用 <span class="math notranslate nohighlight">\(x_ix_j\)</span> 表示，即 <span class="math notranslate nohighlight">\(x_i\)</span> 和 <span class="math notranslate nohighlight">\(x_j\)</span> 都非零时，组合特征 <span class="math notranslate nohighlight">\(x_ix_j\)</span> 才有意义。
从对比的角度，这里只讨论二阶多项式模型。模型的表达式如下:</p>
<blockquote>
<div><img alt="../_images/fm_math1.png" src="../_images/fm_math1.png" />
</div></blockquote>
<p>其中，<span class="math notranslate nohighlight">\(n\)</span> 代表样本的特征数量，<span class="math notranslate nohighlight">\(x_i\)</span> 是第 <span class="math notranslate nohighlight">\(i\)</span> 个特征的值，<span class="math notranslate nohighlight">\(w_0\)</span> 、<span class="math notranslate nohighlight">\(w_i\)</span> 、<span class="math notranslate nohighlight">\(w_{ij}\)</span> 是模型参数。</p>
<p>从多项式模型的公司可以看出，组合特征的参数一共有 <span class="math notranslate nohighlight">\(\frac { n(n-1) }{ 2 }\)</span>  个，任意两个参数都是独立的。然而，在数据稀疏性普遍存在的实际应用场景中，
二次项参数的训练是很困难的。其原因是，每个参数 <span class="math notranslate nohighlight">\(w_{ij}\)</span> 的训练需要大量 <span class="math notranslate nohighlight">\(x_i\)</span> 和 <span class="math notranslate nohighlight">\(x_j\)</span> 都非零的样本；由于样本数据本来就比较稀疏，
满足“ <span class="math notranslate nohighlight">\(x_i\)</span> 和 <span class="math notranslate nohighlight">\(x_j\)</span> 都非零”的样本将会非常少。训练样本的不足，很容易导致参数 <span class="math notranslate nohighlight">\(w_{ij}\)</span> 不准确，最终将严重影响模型的性能。</p>
<p>那么，如何解决二次项参数的训练问题呢？
矩阵分解提供了一种解决思路。一个rating矩阵可以分解为user矩阵和item矩阵，每个user和item都可以采用一个隐向量表示，
比如在下图中的例子中，我们把每个user表示成一个二维向量，同时把每个item表示成一个二维向量，两个向量的点积就是矩阵中user对item的打分。</p>
<blockquote>
<div><img alt="https://awps-assets.meituan.net/mit-x/blog-images-bundle-2016/1a91e67b.png" src="https://awps-assets.meituan.net/mit-x/blog-images-bundle-2016/1a91e67b.png" />
</div></blockquote>
<p>类似地，所有二次项参数 <span class="math notranslate nohighlight">\(w_{ij}\)</span> 可以组成一个对称阵,那么就可以进行分解,换句话说，每个参数 <span class="math notranslate nohighlight">\({ w }_{ ij }=\left&lt; { v }_{ i },{ v }_{ j } \right&gt;\)</span> ，这就是FM模型的核心思想。因此，FM的模型方程为（暂且不讨论FM的高阶形式）</p>
<blockquote>
<div><img alt="../_images/fm_math2.png" src="../_images/fm_math2.png" />
</div></blockquote>
<p>其中，<span class="math notranslate nohighlight">\(v_i\)</span> 是第 <span class="math notranslate nohighlight">\(i\)</span> 维特征的隐向量，<span class="math notranslate nohighlight">\(\left&lt; , \right&gt;\)</span> 代表向量点积。隐向量的长度为 <span class="math notranslate nohighlight">\(k\quad (k&lt;&lt;n)\)</span> ，包含 <span class="math notranslate nohighlight">\(k\)</span> 个描述特征的因子。
根据公式，二次项的参数数量减少为 <span class="math notranslate nohighlight">\(kn\)</span> 个，远少于多项式模型的参数数量。另外，参数因子化使得 <span class="math notranslate nohighlight">\(x_hx_i\)</span> 的参数和 <span class="math notranslate nohighlight">\(x_ix_j\)</span> 的参数不再是相互独立的，
因此我们可以在样本稀疏的情况下相对合理地估计FM的二次项参数。具体来说，<span class="math notranslate nohighlight">\(x_hx_i\)</span> 和 <span class="math notranslate nohighlight">\(x_ix_j\)</span> 的系数分别为 <span class="math notranslate nohighlight">\(\left&lt; { v }_{ h },{ v }_{ j } \right&gt;\)</span> 和 <span class="math notranslate nohighlight">\(\left&lt; { v }_{ i },{ v }_{ j } \right&gt;\)</span> ，它们之间有共同项 <span class="math notranslate nohighlight">\(v_i\)</span>。
也就是说，所有包含“ <span class="math notranslate nohighlight">\(x_i\)</span> 的非零组合特征”（存在某个 <span class="math notranslate nohighlight">\(j≠i\)</span> ，使得 <span class="math notranslate nohighlight">\(x_ix_j≠0\)</span>）的样本都可以用来学习隐向量 <span class="math notranslate nohighlight">\(v_i\)</span> ，这很大程度上避免了数据稀疏性造成的影响。
而在多项式模型中，<span class="math notranslate nohighlight">\(w_{hi}\)</span> 和 <span class="math notranslate nohighlight">\(w_{ij}\)</span> 是相互独立的。</p>
<p>显而易见，公式(2)是一个通用的拟合方程，可以采用不同的损失函数用于解决回归、二元分类等问题，比如可以 采用MSE（Mean Square Error）损失函数来求解回归问题，
也可以采用Hinge/Cross-Entropy损失来求解分类问题。当然，在进行二元分类时，FM的输出需要经过sigmoid变换，这与Logistic回归是一样的。
直观上看，FM的复杂度是 <span class="math notranslate nohighlight">\(O(kn^2)\)</span> 。但是，通过公式(3)的等式，FM的二次项可以化简，其复杂度可以优化到 <span class="math notranslate nohighlight">\(O(kn)\)</span> 。由此可见，FM可以在线性时间对新样本作出预测。</p>
<blockquote>
<div><img alt="../_images/fm_math3.png" src="../_images/fm_math3.png" />
</div></blockquote>
<p>下面给出详细推导：</p>
<blockquote>
<div><img alt="../_images/fm_math4.png" src="../_images/fm_math4.png" />
</div></blockquote>
<p>如果用随机梯度下降SGD（Stochastic Gradient Descent）法学习模型参数。那么，模型各个参数的梯度如下：</p>
<blockquote>
<div><img alt="../_images/fm_math5.png" src="../_images/fm_math5.png" />
</div></blockquote>
<p>其中， <span class="math notranslate nohighlight">\(v_j\)</span> , <span class="math notranslate nohighlight">\(f\)</span> 是隐向量 <span class="math notranslate nohighlight">\(v_j\)</span> 的第 <span class="math notranslate nohighlight">\(f\)</span> 个元素。</p>
<p>FM训练复杂度</p>
<p>由于 <span class="math notranslate nohighlight">\(\sum _{ j=1 }^{ n }{ { v }_{ j,f }{ x }_{ j } }\)</span> 只与 <span class="math notranslate nohighlight">\(f\)</span> 有关，在参数迭代过程中，只需要计算第一次所有 <span class="math notranslate nohighlight">\(f\)</span> 的 <span class="math notranslate nohighlight">\(\sum _{ j=1 }^{ n }{ { v }_{ j,f }{ x }_{ j } }\)</span> ，就能够方便地得到所有 <span class="math notranslate nohighlight">\(v_{i,f}\)</span> 的梯度。显然，
计算所有 <span class="math notranslate nohighlight">\(f\)</span> 的 <span class="math notranslate nohighlight">\(\sum _{ j=1 }^{ n }{ { v }_{ j,f }{ x }_{ j } }\)</span> 的复杂度是 <span class="math notranslate nohighlight">\(O(kn)\)</span> ；已知 <span class="math notranslate nohighlight">\(\sum _{ j=1 }^{ n }{ { v }_{ j,f }{ x }_{ j } }\)</span> 时，计算每个参数梯度的复杂度是 <span class="math notranslate nohighlight">\(O(n)\)</span> ；得到梯度后，更新每个参数的复杂度是 <span class="math notranslate nohighlight">\(O(1)\)</span> ；模型参数一共有 <span class="math notranslate nohighlight">\(nk+n+1\)</span> 个。
因此，FM参数训练的时间复杂度为 <span class="math notranslate nohighlight">\(O(kn)\)</span>。</p>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>1.FM降低了因数据稀疏，导致交叉项参数学习不充分的影响。
每一维特征用 <span class="math notranslate nohighlight">\(k\)</span> 维的隐向量表示，交叉项的参数 <span class="math notranslate nohighlight">\(w_{ij}\)</span> 用对应特征隐向量的内积表示，即 <span class="math notranslate nohighlight">\(\left&lt; { v }_{ i },{ v }_{ j } \right&gt;\)</span> （也可以理解为平滑技术）。这样参数学习由之前学习交叉项参数 <span class="math notranslate nohighlight">\(w_{ij}\)</span> 的过程，转变为学习 <span class="math notranslate nohighlight">\(n\)</span> 个单特征对应 <span class="math notranslate nohighlight">\(k\)</span> 维隐向量的过程。</p>
<p>2.FM提升了模型预估能力
FM可以很方便的计算样本中没有出现的交叉项参数，提升了预估能力</p>
</div>
</div>
<div class="section" id="ffm-field-aware-factorization-machine">
<h2>FFM(Field-aware Factorization Machine)<a class="headerlink" href="#ffm-field-aware-factorization-machine" title="永久链接至标题">¶</a></h2>
<p>FFM是FM的升级模型，通过引入 <span class="math notranslate nohighlight">\(field\)</span> 的概念，FFM把相同性质的特征归于同一个 <span class="math notranslate nohighlight">\(field\)</span> 。以上面的广告分类为例，
“Day=26/11/15”、“Day=1/7/14”、“Day=19/2/15”这三个特征都是代表日期的，可以放到同一个 <span class="math notranslate nohighlight">\(field\)</span> 中。
同理，商品的末级品类编码生成了550个特征，这550个特征都是说明商品所属的品类，因此它们也可以放到同一个 <span class="math notranslate nohighlight">\(field\)</span> 中。
简单来说，同一个categorical特征经过One-Hot编码生成的数值特征都可以放到同一个 <span class="math notranslate nohighlight">\(field\)</span> ，
包括用户性别、职业、品类偏好等。在FFM中，每一维特征 <span class="math notranslate nohighlight">\(x_i\)</span> ，针对其它特征的每一种 <span class="math notranslate nohighlight">\(field\)</span>  <span class="math notranslate nohighlight">\(f_j\)</span>，
都会学习一个隐向量 <span class="math notranslate nohighlight">\(v_{i,f_j}\)</span> 。因此，隐向量不仅与特征相关，也与 <span class="math notranslate nohighlight">\(field\)</span> 相关。
也就是说，“Day=26/11/15”这个特征与“Country”特征和“Ad_type”特征进行关联的时候使用不同的隐向量，
这与“Country”和“Ad_type”的内在差异相符，也是FFM中“field-aware”的由来。</p>
<p>假设样本的 <span class="math notranslate nohighlight">\(n\)</span> 个特征属于 <span class="math notranslate nohighlight">\(f\)</span> 个 <span class="math notranslate nohighlight">\(field\)</span> ，那么FFM的二次项有 <span class="math notranslate nohighlight">\(nf\)</span> 个隐向量。而在FM模型中，每一维特征的隐向量只有一个，也就是总共有 <span class="math notranslate nohighlight">\(n\)</span> 个。
FM可以看作FFM的特例，是把所有特征都归属到一个 <span class="math notranslate nohighlight">\(field\)</span> 时的FFM模型。根据FFM的 <span class="math notranslate nohighlight">\(field\)</span> 敏感特性，可以导出其模型方程。</p>
<blockquote>
<div><img alt="../_images/ffm_math1.png" src="../_images/ffm_math1.png" />
</div></blockquote>
<p>其中，<span class="math notranslate nohighlight">\(f_j\)</span> 是第 <span class="math notranslate nohighlight">\(j\)</span> 个特征所属的 <span class="math notranslate nohighlight">\(field\)</span> 。如果隐向量的长度为 <span class="math notranslate nohighlight">\(k\)</span> ，那么FFM的二次参数有 <span class="math notranslate nohighlight">\(nfk\)</span> 个，远多于FM模型的 <span class="math notranslate nohighlight">\(nk\)</span> 个。
此外，由于隐向量与 <span class="math notranslate nohighlight">\(field\)</span> 相关，FFM二次项并不能够化简，其预测复杂度是 <span class="math notranslate nohighlight">\(O(kn^2)\)</span> 。 <strong>所以ffm容易造成过拟合，尽量将k值取小一点，一般取3~5</strong> 。</p>
<p>下面以一个例子简单说明FFM的特征组合方式。输入记录如下</p>
<blockquote>
<div><img alt="../_images/ffm1.png" src="../_images/ffm1.png" />
</div></blockquote>
<p>这条记录可以编码成5个特征，其中“Genre=Comedy”和“Genre=Drama”属于同一个field，“Price”是数值型，不用One-Hot编码转换。
为了方便说明FFM的样本格式，我们将所有的特征和对应的 <span class="math notranslate nohighlight">\(field\)</span> 映射成整数编号。</p>
<blockquote>
<div><img alt="../_images/ffm2.png" src="../_images/ffm2.png" />
</div></blockquote>
<p>那么，FFM的组合特征有10项，如下图所示。</p>
<blockquote>
<div><img alt="../_images/ffm4.png" src="../_images/ffm4.png" />
</div></blockquote>
<p>其中，红色是 <span class="math notranslate nohighlight">\(field\)</span> 编号，蓝色是特征编号，绿色是此样本的特征取值。二次项的系数是通过与特征 <span class="math notranslate nohighlight">\(field\)</span> 相关的隐向量点积得到的，二次项共有 <span class="math notranslate nohighlight">\(\frac { n(n-1) }{ 2 }\)</span> 个。</p>
</div>
<div class="section" id="deepfm">
<h2>DeepFm<a class="headerlink" href="#deepfm" title="永久链接至标题">¶</a></h2>
<div class="section" id="id3">
<h3>DeepFm的提出背景<a class="headerlink" href="#id3" title="永久链接至标题">¶</a></h3>
<p>CTR预估数据特点：</p>
<ul class="simple">
<li><ol class="arabic simple">
<li><p>输入中包含类别型和连续型数据。类别型数据需要one-hot,连续型数据可以先离散化再one-hot，也可以直接保留原值</p></li>
</ol>
</li>
<li><ol class="arabic simple" start="2">
<li><p>维度非常高</p></li>
</ol>
</li>
<li><ol class="arabic simple" start="3">
<li><p>数据非常稀疏</p></li>
</ol>
</li>
<li><ol class="arabic simple" start="4">
<li><p>特征按照Field分组</p></li>
</ol>
</li>
</ul>
<p>CTR预估重点在于学习组合特征。注意，组合特征包括二阶、三阶甚至更高阶的，阶数越高越复杂，越不容易学习。Google的论文研究得出结论：
<strong>高阶和低阶的组合特征都非常重要，同时学习到这两种组合特征的性能要比只考虑其中一种的性能要好</strong> 。</p>
<p>那么关键问题转化成： <strong>如何高效的提取这些组合特征</strong> 。一种办法就是引入领域知识人工进行特征工程。这样做的弊端是高阶组合特征非常难提取，
会耗费极大的人力。而且，有些组合特征是隐藏在数据中的，即使是专家也不一定能提取出来，比如著名的“尿布与啤酒”问题。</p>
<p><strong>线性模型的局限性：无法提取高阶的组合特征</strong> ，比如 LR，FTRL</p>
<p><strong>FM模型：仅仅从理论上解决了低阶和高阶组合特征提取的问题。但是实际应用中受限于计算复杂度，一般也就只考虑到2阶交叉特征</strong>。</p>
<dl class="simple">
<dt>其它的融合深度学习的模型：无论是FNN还是PNN，他们都有一个绕不过去的缺点：<strong>对于低阶的组合特征，学习到的比较少</strong>；</dt><dd><p>Google意识到了这个问题，为了同时学习低阶和高阶组合特征，提出了Wide&amp;Deep模型。它混合了一个线性模型（Wide part）和Deep模型(Deep part)。
这两部分模型需要不同的输入，而Wide part部分的输入，依旧依赖人工特征工程。</p>
</dd>
</dl>
<p>DeepFM在Wide&amp;Deep的基础上进行改进，成功解决了这两个问题，并做了一些改进，其优势/优点如下：</p>
<blockquote>
<div><ul class="simple">
<li><ol class="arabic simple">
<li><p>不需要预训练FM得到隐向量</p></li>
</ol>
</li>
<li><ol class="arabic simple" start="2">
<li><p>不需要人工特征工程</p></li>
</ol>
</li>
<li><ol class="arabic simple" start="3">
<li><p>能同时学习低阶和高阶的组合特征</p></li>
</ol>
</li>
<li><ol class="arabic simple" start="4">
<li><p>FM模块和Deep模块共享Feature Embedding部分，可以更快的训练，以及更精确的训练学习</p></li>
</ol>
</li>
</ul>
</div></blockquote>
<p>将DNN与FM进行一个合理的融合总的来说有两种形式，一是 <strong>串行结构</strong> ，二是 <strong>并行结构</strong> 。</p>
<blockquote>
<div><img alt="../_images/deepfmbk1.jpg" src="../_images/deepfmbk1.jpg" />
<img alt="../_images/deepfmbk2.jpg" src="../_images/deepfmbk2.jpg" />
</div></blockquote>
<p>串行结构代表是FNN，<strong>DeepFM就是并行结构中的一种典型代表</strong> 。</p>
</div>
<div class="section" id="id4">
<h3>DeepFm原理<a class="headerlink" href="#id4" title="永久链接至标题">¶</a></h3>
<p>首先给出论文模型图，由于文章画的很好，就直接贴图了：</p>
<blockquote>
<div><img alt="../_images/deepfm1.jpg" src="../_images/deepfm1.jpg" />
</div></blockquote>
<p>可以看到，整个模型大体分为两部分：FM和DNN。<strong>简单叙述一下模型的流程</strong>：借助FNN的思想，利用FM进行embedding，
之后的wide和deep模型共享embedding之后的结果。DNN的输入完全和FNN相同
（这里不用预训练，直接把embedding层看NN的一个隐含层），而通过一定方式组合后，
模型在wide上完全模拟出了FM的效果（后面会有推导过程），最后将DNN和FM的结果组合后激活输出。</p>
<p><strong>DeepFM的预测结果可以写为</strong> ：</p>
<blockquote>
<div><img alt="../_images/deepfm2.jpg" src="../_images/deepfm2.jpg" />
</div></blockquote>
<div class="section" id="embeddingfm">
<h4>embedding和FM<a class="headerlink" href="#embeddingfm" title="永久链接至标题">¶</a></h4>
<p>模型在embedding的过程中，也是借助了FM的方式, 通过隐向量与X的内积运算，将稀疏的X向量转化为K维度的稠密向量，
即得到embedding向量数据，用于接下来DNN的输入以及FM的输出，回顾二阶FM的公式如下：</p>
<blockquote>
<div><img alt="../_images/deepfm3.jpg" src="../_images/deepfm3.jpg" />
</div></blockquote>
<p>其中向量V就是对应特征的隐向量。而在目前很多的DNN模型中，都是借助了FM这种形式来做的embedding，具体推导如下：</p>
<blockquote>
<div><img alt="../_images/deepfm4.jpg" src="../_images/deepfm4.jpg" />
</div></blockquote>
<p>由于CTR的输入一般是稀疏的，因此在第一层隐含层之前，引入一个嵌入层来完成将输入向量压缩到低维稠密向量，
嵌入层(embedding layer)的结构如上图所示。当前网络结构有两个有趣的特性：</p>
<ul class="simple">
<li><ol class="arabic simple">
<li><p><strong>尽管不同field的输入长度不同，但是embedding之后向量的长度均为K</strong></p></li>
</ol>
</li>
<li><ol class="arabic simple" start="2">
<li><p><strong>在FM里得到的隐变量Vik现在作为了嵌入层网络的权重，通过FM里面的二阶运算，将稀疏的X输入向量压缩为低纬度的稠密向量</strong></p></li>
</ol>
</li>
</ul>
<p>这里的第二点如何理解呢，假设我们的k=5，首先，对于输入的一条记录，同一个field 只有一个位置是1，那么在由输入得到dense vector的过程中，
输入层只有一个神经元起作用，得到的dense vector其实就是输入层到embedding层该神经元相连的五条线的权重，
即vi1，vi2，vi3，vi4，vi5。这五个值组合起来就是我们在FM中所提到的Vi。在FM部分和DNN部分，这一块是共享权重的，
对同一个特征来说，得到的Vi是相同的。</p>
<p>具体栗子如下：</p>
<p>这里k表示隐向量的维数，Vij表示第i个特征embeding之后在隐向量的第j维。假设我已经给出了V矩阵，</p>
<blockquote>
<div><img alt="../_images/deepfm5.jpg" src="../_images/deepfm5.jpg" />
</div></blockquote>
<p>其中第5-15个特征是同一个field经过one-hot编码后的表示，这是隐向量按列排成矩阵，同时，
它也可看作embedding层的参数矩阵，按照神经网络前向传播的方式，embedding后的该slot下的向量值应该表示为：</p>
<blockquote>
<div><img alt="../_images/deepfm6.jpg" src="../_images/deepfm6.jpg" />
</div></blockquote>
<p>可以看到这个结果就是一个5维的向量，而这个普通的神经网络传递时怎么和FM联系到一起的，仔细观察这个式子可以发现，由于是离散化或者one-hot之后的X，
所以对于每一个field的特征而言，截断向量X都只有一个值为1，其他都为0。那么假设上述slot的V中，j为7的特征值为1，那么矩阵相乘之后的结果为：</p>
<blockquote>
<div><img alt="../_images/deepfm7.jpg" src="../_images/deepfm7.jpg" />
</div></blockquote>
<p>从结果中可以看到，实质上，每个slot在embedding后，其结果都是one-hot后有值的那一维特征所对应的隐向量，FNN也是同样如此。看到这里，
在来解释模型中FM部分是如何借助这种方式得到的。回到模型的示意图，可以看到在FM测，是对每两个embedding向量做内积，那么我们来看，
假设两个slot分别是第7和第20个特征值为1：</p>
<blockquote>
<div><img alt="../_images/deepfm8.jpg" src="../_images/deepfm8.jpg" />
</div></blockquote>
<p>是不是感觉特别熟悉，没错，这个乘积的结果就是FM中二阶特征组合的其中一例，而对于所有非0组合（embedding向量组合）求和之后，
就是FM中所有二阶特征的部分，这就是模型中FM部分的由来。</p>
<p>当然，<strong>FM中的一阶特征，则直接在embedding之前对于特征进行组合即可</strong>。</p>
<p>模型当中的FM中涉及的参数同样会参与到梯度下降中，也就是说embedding层不会只是初始化使用，实质上这里可以理解为是直接用神经网络来做了embedding，
无疑他比FM本身的embedding能力来得更强。</p>
</div>
</div>
<div class="section" id="dnn">
<h3>DNN<a class="headerlink" href="#dnn" title="永久链接至标题">¶</a></h3>
<p>DNN部分依然采用了全连接的深度神经网络的方式，只是在最后激活输出时，会对FM和DNN的结果一起做embedding</p>
<p>超参数建议：</p>
<img alt="../_images/deepfm9.jpg" src="../_images/deepfm9.jpg" />
<p><strong>PS: constant效果最好，就是隐藏层每一层的神经元的数量 相同</strong> (神经元总个数的不变的情况下，平均到每一层效果最好)。</p>
</div>
</div>
<div class="section" id="wide-deep">
<h2>Wide&amp;Deep<a class="headerlink" href="#wide-deep" title="永久链接至标题">¶</a></h2>
<p>LR+DNN</p>
<p>Wide &amp; Deep 模型的核心思想是结合 <strong>线性模型的记忆能力</strong> 和 <strong>DNN 模型的泛化能力</strong> ，从而提升整体模型性能。</p>
<div class="section" id="motivation">
<h3>一、Motivation<a class="headerlink" href="#motivation" title="永久链接至标题">¶</a></h3>
<p>推荐系统的主要挑战之一，是同时解决Memorization和Generalization，理解这两个概念是理解整个模型思路的关键，下面分别进行解释。</p>
<ul>
<li><p>Memorization</p>
<blockquote>
<div><p>面对拥有大规模离散sparse特征的CTR预估问题时，将特征进行非线性转换，然后再使用线性模型是在业界非常普遍的做法，最流行的即「LR+特征叉乘」。
Memorization <strong>通过一系列人工的特征叉乘（cross-product）来构造这些非线性特征，捕捉sparse特征之间的高阶相关性，即“记忆” 历史数据中曾共同出现过的特征对</strong>。</p>
<p>例如，特征1——专业: {计算机、人文、其他}，特征2——下载过音乐《消愁》:{是、否}，这两个特征one-hot后的特征维度分别为3维与2维，
对应的叉乘结果是特征3——专业☓下载过音乐《消愁》:{计算机∧是，计算机∧否，人文∧是，人文∧否，其他∧是，其他∧否}。</p>
<p><strong>典型代表是LR模型，使用大量的原始sparse特征和叉乘特征作为输入</strong> ，很多原始的dense特征通常也会被分桶离散化构造为sparse特征。这种做法的优点是模型可解释高，
实现快速高效，特征重要度易于分析，在工业界已被证明是很有效的。Memorization的缺点是：</p>
<ul class="simple">
<li><ol class="loweralpha simple">
<li><p>需要更多的人工设计；</p></li>
</ol>
</li>
<li><ol class="loweralpha simple" start="2">
<li><p>可能出现过拟合。可以这样理解：如果将所有特征叉乘起来，那么几乎相当于纯粹记住每个训练样本，这个极端情况是最细粒度的叉乘，我们可以通过构造更粗粒度的特征叉乘来增强泛化性；</p></li>
</ol>
</li>
<li><ol class="loweralpha simple" start="3">
<li><p>无法捕捉训练数据中未曾出现过的特征对。例如上面的例子中，如果每个专业的人都没有下载过《消愁》，那么这两个特征共同出现的频次是0，模型训练后的对应权重也将是0；</p></li>
</ol>
</li>
</ul>
</div></blockquote>
</li>
<li><p>Generalization</p>
<blockquote>
<div><p>Generalization <strong>为sparse特征学习低维的dense embeddings 来捕获特征相关性</strong>，学习到的embeddings 本身带有一定的语义信息。可以联想到NLP中的词向量，不同词的词向量有相关性，
因此文中也称Generalization是基于相关性之间的传递。这类模型的代表是DNN和FM。</p>
<p>Generalization的优点是更少的人工参与，对历史上没有出现的特征组合有更好的泛化性 。但在推荐系统中，当user-item matrix非常稀疏时，例如有和独特爱好的users以及很小众的items，
NN很难为users和items学习到有效的embedding。这种情况下，大部分user-item应该是没有关联的，但dense embedding 的方法还是可以得到对所有 user-item pair 的非零预测，
因此导致 over-generalize并推荐不怎么相关的物品。此时Memorization就展示了优势，它可以“记住”这些特殊的特征组合。</p>
<p><strong>Memorization根据历史行为数据，产生的推荐通常和用户已有行为的物品直接相关的物品。而Generalization会学习新的特征组合，提高推荐物品的多样性。</strong></p>
<p>Wide &amp; Deep Learning，其中Wide &amp; Deep分别对应Memorization &amp; Generalization。</p>
</div></blockquote>
</li>
</ul>
</div>
<div class="section" id="model">
<h3>二、Model<a class="headerlink" href="#model" title="永久链接至标题">¶</a></h3>
<ul>
<li><p>模型结构</p>
<blockquote>
<div><p>Wide &amp; Deep模型结合了LR和DNN，其框架图如下所示。</p>
<img alt="../_images/w_d1.jpg" src="../_images/w_d1.jpg" />
<p>Wide 该部分是广义线性模型，即:</p>
<p><span class="math notranslate nohighlight">\(y\quad =\quad { w }^{ T }\left[ x,\phi (x) \right] +b\)</span> 其中 <span class="math notranslate nohighlight">\(x\)</span> 和 <span class="math notranslate nohighlight">\(\phi (x)\)</span> 表示 <strong>原始特征</strong> 与叉乘特征。</p>
<p>Deep 该部分是前馈神经网络，网络会对一些sparse特征（如ID类特征）学习一个低维的dense embeddings（维度量级通常在O(10)到O(100)之间），
然后和一些原始dense特征一起作为网络的输入。每一层隐层计算：</p>
<div class="math notranslate nohighlight" id="equation-data-mining-ctr-1">
<span class="eqno">()<a class="headerlink" href="#equation-data-mining-ctr-1" title="公式的永久链接">¶</a></span>\[{ a }^{ l+1 }=f({ W }^{ l }{ a }^{ l }+{ b }^{ l })\]</div>
<p>其中 <span class="math notranslate nohighlight">\(a^l\)</span> , <span class="math notranslate nohighlight">\(b^l\)</span> , <span class="math notranslate nohighlight">\(W^l\)</span> 是第 <span class="math notranslate nohighlight">\(l\)</span> 层的激活值、偏置和权重，<span class="math notranslate nohighlight">\(f\)</span> 是激活函数。</p>
<p>损失函数 模型选取logistic loss作为损失函数，此时Wide &amp; Deep最后的预测输出为：</p>
<div class="math notranslate nohighlight" id="equation-data-mining-ctr-2">
<span class="eqno">()<a class="headerlink" href="#equation-data-mining-ctr-2" title="公式的永久链接">¶</a></span>\[p(y=1|x)\quad =\quad \sigma ({ w }_{ wide }^{ T }\left[ x,\phi (x) \right] +{ w }_{ deep }^{ T }{ a }^{ { l }_{ f } }+b)\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\sigma\)</span> 表示sigmoid函数， <span class="math notranslate nohighlight">\(\phi({x})\)</span> 表示叉乘特征， <span class="math notranslate nohighlight">\({a}^{l_f}\)</span> 表示NN最后一层激活值。</p>
</div></blockquote>
</li>
<li><p>联合训练</p>
<blockquote>
<div><p>联合训练（Joint Training）和集成（Ensemble）是不同的，集成是每个模型单独训练，再将模型的结果汇合。相比联合训练，集成的每个独立模型都得学得足够好才有利于随后的汇合，因此每个模型的model size也相对更大。
而联合训练的wide部分只需要作一小部分的特征叉乘来弥补deep部分的不足，不需要 一个full-size 的wide 模型。</p>
</div></blockquote>
</li>
</ul>
</div>
</div>
<div class="section" id="deep-cross">
<h2>Deep&amp;Cross<a class="headerlink" href="#deep-cross" title="永久链接至标题">¶</a></h2>
<p>Deep&amp;Cross是Google 对 Wide &amp; Deep工作的一个后续研究，文中提出 Deep &amp; Cross Network，将Wide部分替换为由特殊网络结构实现的Cross，
自动构造有限高阶的交叉特征，并学习对应权重，告别了繁琐的人工叉乘。</p>
<p>Deep &amp; Cross 由斯坦福与Google联合发表在AdKDD 2017上的论文《Deep &amp; Cross Network for Ad Click Predictions》。</p>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>AdKDD是SIGKDD在Computational Advertising领域的一个workshop，从2007年举办至今，发表了很多Ad领域的经典工作，例如2013年Facebook的GBDT+LR。
总体来说，这个workshop能够较好地反映这个行业的巨头们在Ad方向上的关注和研发重点，发表的工作实用性很强。</p>
</div>
<p><strong>Motivation</strong></p>
<blockquote>
<div><p>Wide &amp; Deep 的结构来同时实现Memorization与Generalization，但是在Wide部分，仍然需要人工地设计特征叉乘。
面对高维稀疏的特征空间、大量的可组合方式，基于人工先验知识虽然可以缓解一部分压力，但仍需要不小的人力和尝试成本，
并且很有可能遗漏一些重要的交叉特征。FM可以自动组合特征，但也仅限于二阶叉乘。能否告别人工组合特征，
并且自动学习高阶的特征组合呢？Deep &amp; Cross 即是对此的一个尝试。</p>
</div></blockquote>
<p><strong>Model</strong></p>
<p>Deep &amp; Cross的网络结构如图</p>
<blockquote>
<div><img alt="../_images/d_c1.jpg" src="../_images/d_c1.jpg" />
</div></blockquote>
<p>首先对原始特征做如下处理：</p>
<blockquote>
<div><ul>
<li><ol class="loweralpha simple">
<li><p>对sparse特征进行embedding，<strong>对于multi-hot的sparse特征，embedding之后再做一个简单的average pooling</strong> ；</p></li>
</ol>
</li>
<li><ol class="loweralpha simple" start="2">
<li><p>对dense特征归一化，然后和embedding特征拼接，作为随后Cross层与Deep层的共同输入，即：</p></li>
</ol>
<blockquote>
<div><div class="math notranslate nohighlight" id="equation-data-mining-ctr-3">
<span class="eqno">()<a class="headerlink" href="#equation-data-mining-ctr-3" title="公式的永久链接">¶</a></span>\[{ x }_{ 0 }=[~ { x }_{ embed,1 }^{ T },~ { x }_{ embed,2 }^{ T },~ ...,~ { x }_{ embed,k }^{ T },~ { x }_{ dense }^{ T }]^{ T }\]</div>
</div></blockquote>
</li>
</ul>
</div></blockquote>
<ul>
<li><p>Cross Layer</p>
<blockquote>
<div><p><strong>Cross的目的是以一种显示、可控且高效的方式，自动构造有限高阶交叉特征</strong>。Cross结构如上图左侧所示，其中第 <span class="math notranslate nohighlight">\(l+1\)</span> 层输出为：</p>
<blockquote>
<div><img alt="../_images/d_c2.jpg" src="../_images/d_c2.jpg" />
</div></blockquote>
<dl>
<dt>即：</dt><dd><div class="math notranslate nohighlight" id="equation-data-mining-ctr-4">
<span class="eqno">()<a class="headerlink" href="#equation-data-mining-ctr-4" title="公式的永久链接">¶</a></span>\[{ x }_{ l+1 }={ x }_{ 0 }{ x }_{ l }^{ T }{ w }_{ l }+b+{ x }_{ l }=f({ x }_{ l },{ w }_{ l },{ b }_{ l })+{ x }_{ l },\quad 其中{ x }_{ l+1 }{ ;x }_{ l };{ x }_{ 0 }\in R\]</div>
</dd>
</dl>
<p>Cross Layer 设计的巧妙之处全部体现在上面的计算公式中，我们先看一些明显的细节：</p>
<ul>
<li><ol class="arabic simple">
<li><p>每层的神经元个数都相同，都等于输入 <span class="math notranslate nohighlight">\({x}_0\)</span>  的维度 <span class="math notranslate nohighlight">\(d\)</span> ，也即每层的输入输出维度都是相等的；</p></li>
</ol>
</li>
<li><ol class="arabic simple" start="2">
<li><p>受残差网络（Residual Network）结构启发，每层的函数 <span class="math notranslate nohighlight">\(f\)</span> 拟合的是 <span class="math notranslate nohighlight">\({x}_{l+1}-{x}_l\)</span> 的残差，</p></li>
</ol>
<blockquote>
<div><p>残差网络有很多优点，其中一点是处理梯度消失的问题，使网络可以“更深”.</p>
</div></blockquote>
</li>
</ul>
<p>举个栗子说明上述公式干了什么：</p>
<p>假设Cross有 <span class="math notranslate nohighlight">\(2\)</span> 层，<span class="math notranslate nohighlight">\({ x }_{ 0 }=\begin{bmatrix} { x }_{ 0,1 } \\ { x }_{ 0,2 } \end{bmatrix}\)</span> ,为了方便讨论不考虑
偏置项，即设定偏置项 <span class="math notranslate nohighlight">\(b_i=0\)</span>, 则：</p>
<img alt="../_images/d_c3.png" src="../_images/d_c3.png" />
<p>最后得到 <span class="math notranslate nohighlight">\(y_{cross}={x}_2^T*{w}_{cross} \in \mathbb{R}\)</span> 参与到最后的 loss计算。</p>
<p>可以看到 <span class="math notranslate nohighlight">\({x}_1\)</span> 包含了原始特征 <span class="math notranslate nohighlight">\(x_{0,1},x_{0,2}\)</span> 从一阶到二阶的所有可能叉乘组合，
而 <span class="math notranslate nohighlight">\({x}_2\)</span> 包含了其从一阶到三阶的所有可能叉乘组合。现在知道cross layer的厉(niu)害(x)之处了吧,总结下Cross的设计：</p>
<ul class="simple">
<li><ol class="arabic simple">
<li><p><strong>有限高阶</strong>：叉乘阶数由网络深度决定，深度 <span class="math notranslate nohighlight">\(L_c\)</span> 对应最高 <span class="math notranslate nohighlight">\(L_c+1\)</span>  阶的叉乘</p></li>
</ol>
</li>
<li><ol class="arabic simple" start="2">
<li><p><strong>自动叉乘</strong>：Cross输出包含了原始特征从一阶（即本身）到 <span class="math notranslate nohighlight">\(L_c+1\)</span>  阶的所有叉乘组合，而模型参数量仅仅随输入维度成线性增长： <span class="math notranslate nohighlight">\(2*d*L_c\)</span></p></li>
</ol>
</li>
<li><ol class="arabic simple" start="3">
<li><p><strong>参数共享</strong>：不同叉乘项对应的权重不同，但并非每个叉乘组合对应独立的权重（指数数量级）， 通过参数共享，Cross有效降低了参数量。此外，参数共享还使得模型有更强的泛化性和鲁棒性。例如，如果独立训练权重，当训练集中 <span class="math notranslate nohighlight">\(x_i \ne 0\wedge x_j \ne0\)</span> 这个叉乘特征没有出现 ，对应权重肯定是零，而参数共享则不会，类似地，数据集中的一些噪声可以由大部分正常样本来纠正权重参数的学习</p></li>
</ol>
</li>
</ul>
</div></blockquote>
</li>
<li><p>联合训练</p>
<blockquote>
<div><p>模型的Deep 部分如图1右侧部分所示，DCN拼接Cross 和Deep的输出，采用logistic loss作为损失函数，进行联合训练，这些细节与Wide &amp; Deep几乎是一致的，
在这里不再展开论述。另外，原论文中也在目标函数中加入L2正则防止过拟合。</p>
</div></blockquote>
</li>
<li><p>参数分析</p>
<blockquote>
<div><p>设初始输入 <span class="math notranslate nohighlight">\({x}_0\)</span> 维度为 <span class="math notranslate nohighlight">\(d\)</span> ，Deep和Cross层数分别为 <span class="math notranslate nohighlight">\(L_{cross}\)</span> 和 <span class="math notranslate nohighlight">\(L_{deep}\)</span> ，为便于分析，设Deep每层神经元个数为 <span class="math notranslate nohighlight">\(m\)</span> ，则两部分的参数量为：</p>
<div class="math notranslate nohighlight" id="equation-data-mining-ctr-5">
<span class="eqno">()<a class="headerlink" href="#equation-data-mining-ctr-5" title="公式的永久链接">¶</a></span>\[Cross: d*L_{cross}*2 ~~~VS~~~ Deep: (d*m+m)+(m^2+m)*(L_{deep}-1)\]</div>
<p>可以看到Cross的参数量随 d 增大仅呈“线性增长”！相比于Deep部分，对整体模型的复杂度影响不大，这得益于Cross的特殊网络设计，
对于模型在业界落地并实际上线来说，这是一个相当诱人的特点。</p>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>这里有一点很值得留意，前面介绍过，将dense特征和embedding特征拼接后作为Cross层和Deep层的共同输入。
这对于Deep层是合理的，但我们知道人工交叉特征基本是对原始sparse特征进行叉乘，那为何不直接用原始sparse特征作为Cross的输入呢？
联系这里介绍的Cross设计，每层layer的节点数都与Cross的输入维度一致的，直接使用大规模高维的sparse特征作为输入，
会导致极大地增加Cross的参数量。当然其实直接拿原始sparse特征喂给Cross层，才是真正“省去人工叉乘”的更完美实现，
但是现实条件不太允许。所以将高维sparse特征转化为低维的embedding，再喂给Cross，实则是一种trade-off的可行选择。</p>
<ol class="arabic simple">
<li><p>论文提出一种新型的交叉网络结构 DCN，其中 Cross 可以显示、自动地构造有限高阶的特征叉乘，从而在一定程度上告别人工特征叉乘</p></li>
<li><p>Cross部分的复杂度与输入维度呈线性关系，相比DNN非常节约内存。</p></li>
</ol>
</div>
</div></blockquote>
</li>
</ul>
</div>
<div class="section" id="din">
<h2>阿里DIN<a class="headerlink" href="#din" title="永久链接至标题">¶</a></h2>
<p>Deep Interest Network(DIN) <strong>针对电子商务领域(e-commerce industry)的CTR预估</strong>，
<strong>重点在于充分利用/挖掘用户历史行为数据中的信息</strong>。</p>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>互联网电子商务领域，数据特点：Diversity、Local Activation</p>
<ul class="simple">
<li><dl class="simple">
<dt>Diversity:</dt><dd><p>用户在访问电商网站时会对多种商品都感兴趣。也就是用户的兴趣非常的广泛。</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Local Activation：</dt><dd><p>用户是否会点击推荐给他的商品，仅仅取决于历史行为数据中的一小部分，而不是全部。</p>
</dd>
</dl>
</li>
</ul>
<dl>
<dt>ep:</dt><dd><img alt="https://pic2.zhimg.com/80/v2-c58bedc2f95a830de0b13762fed557b5_hd.jpg" src="https://pic2.zhimg.com/80/v2-c58bedc2f95a830de0b13762fed557b5_hd.jpg" />
</dd>
</dl>
<p>Diversity：一个年轻的母亲，从他的历史行为中，我们可以看到她的兴趣非常广泛：羊毛衫、手提袋、耳环、童装、运动装等等。</p>
<p>Local Activation：一个爱游泳的人，他之前购买过travel book、ice cream、potato chips、swimming cap。
当前给他推荐的商品(或者说是广告Ad)是goggle(护目镜)。那么他是否会点击这次广告，
跟他之前是否购买过薯片、书籍、冰激凌一丁点关系也没有！而是与他之前购买过游泳帽有关系。
也就是说在这一次CTR预估中，部分历史数据(swimming cap)起了决定作用，而其他的基本都没啥用。</p>
</div>
<p>DIN 给出解决方案：</p>
<blockquote>
<div><ul class="simple">
<li><p>使用用户兴趣分布来表示用户多种多样的兴趣爱好；</p></li>
<li><p>使用Attention机制来实现Local Activation；</p></li>
<li><p>针对模型训练，提出了Dice激活函数，自适应正则，显著提升了模型性能与收敛速度。</p></li>
</ul>
</div></blockquote>
<p><strong>DIN同时对Diversity和Local Activation进行建模</strong></p>
<blockquote>
<div><ul class="simple">
<li><dl class="simple">
<dt>针对Diversity：</dt><dd><p>针对用户广泛的兴趣，DIN用an interest distribution去表示。</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>针对Local Activation：</dt><dd><p>DIN借鉴机器翻译中的Attention机制，设计了一种attention-like network structure，
针对当前候选Ad，去局部的激活(Local Activate)相关的历史兴趣信息。和当前候选Ad相关性越高的历史行为，
会获得更高的attention score，从而会主导这一次预测。</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>当DNN深度比较深(参数非常多)，输入又非常稀疏的时候，很容易过拟合。DIN提出Adaptive regularizaion来</dt><dd><p>防止过拟合，效果显著。</p>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
<p>DIN方法也可以应用于其他有丰富用户行为数据的场景，比如：</p>
<blockquote>
<div><ul class="simple">
<li><p>电子商务中的个性化推荐</p></li>
<li><p>社交网络中的信息推流排序(feeds ranking)</p></li>
</ul>
</div></blockquote>
<div class="section" id="id5">
<h3>系统概览<a class="headerlink" href="#id5" title="永久链接至标题">¶</a></h3>
<img alt="https://pic1.zhimg.com/80/v2-a23baf1a2a8b5c0cee0dbde204985c70_hd.jpg" src="https://pic1.zhimg.com/80/v2-a23baf1a2a8b5c0cee0dbde204985c70_hd.jpg" />
<p>阿里推荐系统工作流程就像上图所示：</p>
<blockquote>
<div><ul class="simple">
<li><ol class="arabic simple">
<li><p>检查用户历史行为数据</p></li>
</ol>
</li>
<li><ol class="arabic simple" start="2">
<li><p>使用matching module产生候选ads</p></li>
</ol>
</li>
<li><ol class="arabic simple" start="3">
<li><p>通过ranking module得到候选ads的点击概率，并根据概率排序得到推荐列表</p></li>
</ol>
</li>
<li><ol class="arabic simple" start="4">
<li><p>记录下用户在当前展示广告下的反应(点击与否)</p></li>
</ol>
</li>
</ul>
</div></blockquote>
<p>这是一个闭环的系统，对于用户行为数据(User Behavior Data)，系统自己生产并消费。</p>
</div>
<div class="section" id="id6">
<h3>训练数据<a class="headerlink" href="#id6" title="永久链接至标题">¶</a></h3>
<p>前面提到，电子商务领域，充分利用User Behavior Data非常关键，而它又有着非常显著的特点：</p>
<blockquote>
<div><ul class="simple">
<li><p>Diversity. 兴趣爱好非常广泛</p></li>
<li><p>Local Activation. 历史行为中部分数据主导是否会点击候选广告</p></li>
</ul>
</div></blockquote>
<p>还有的特点，就是CTR中输入普遍存在的特点：</p>
<blockquote>
<div><ul class="simple">
<li><p>高纬度</p></li>
<li><p>非常稀疏</p></li>
</ul>
</div></blockquote>
<p>CTR中一旦涉及到用户行为数据，还有一个特点：</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>特征往往都是multi-hot的稀疏ids。</strong></p></li>
</ul>
</div></blockquote>
<p>也就是：多值离散特征。在电子商务上的例子就是：用户购买过的good_id有多个，购买过的shop_id也有多个，
而这也直接导致了每个用户的历史行为id长度是不同的。</p>
<p>为了得到一个固定长度的Embedding Vector表示，一般的做法是在Embedding Layer后面增加一个Pooling Layer。
Pooling可以用sum或average。最终得到一个固定长度的Embedding Vector，是用户兴趣的一个抽象表示，
<strong>缺点是会损失一些信息。</strong></p>
<p>DIN使用Attention机制来解决这个问题。Attention机制来源于Neural Machine Translation(NMT)。
DIN使用Attention机制去更好的建模局部激活。
<strong>也就是说：在Embedding Layer -&gt; Pooling Layer得到用户兴趣表示的时候，赋予不同的历史行为不同的权重，实现局部激活。</strong>
从最终反向训练的角度来看，就是根据当前的候选广告，来反向的激活用户历史的兴趣爱好，赋予不同历史行为不同的权重。</p>
</div>
<div class="section" id="id7">
<h3>特征设计<a class="headerlink" href="#id7" title="永久链接至标题">¶</a></h3>
<p>论文中作者把特征分为四大类，并没有进行特征组合/交叉特征。而是通过DNN去学习特征间的交互信息。特征如下：</p>
<img alt="https://pic1.zhimg.com/80/v2-39dcdd90fe00b48b50967543fe0bf980_hd.jpg" src="https://pic1.zhimg.com/80/v2-39dcdd90fe00b48b50967543fe0bf980_hd.jpg" />
<p>可以看到特征主要包括：用户特征、用户行为特征、广告特征、上下文特征。
其中，只有用户行为特征中会出现multi-hot
原因就是一个用户会购买多个good_id,也会访问多个shop_id，另一个表现就是这样导致了每个用户的样本长度都是不同的.</p>
</div>
<div class="section" id="id8">
<h3>评价指标<a class="headerlink" href="#id8" title="永久链接至标题">¶</a></h3>
<p>模型使用的评价指标是GAUC，我们先来看一下GAUC的计算公式：</p>
<img alt="https://pic1.zhimg.com/80/v2-2387b4e753f67f78b74cc2687e6668a8_hd.jpg" src="https://pic1.zhimg.com/80/v2-2387b4e753f67f78b74cc2687e6668a8_hd.jpg" />
<p>我们首先要肯定的是，AUC是要分用户看的，我们的模型的预测结果，只要能够保证对每个用户来说，他想要的结果排在前面就好了。</p>
<p>假设有两个用户A和B，每个用户都有10个商品，10个商品中有5个是正样本，我们分别用TA，TB，FA，FB来表示两个用户的正样本和负样本。
也就是说，20个商品中有10个是正样本。假设模型预测的结果大小排序依次为TA，FA，TB，FB。如果把两个用户的结果混起来看，
AUC并不是很高，因为有5个正样本排在了后面，但是分开看的话，每个用户的正样本都排在了负样本之前，AUC应该是1。
显然，分开看更容易体现模型的效果，这样消除了用户本身的差异。</p>
<p>但是上文中所说的差异是在用户点击数即样本数相同的情况下说的。还有一种差异是用户的展示次数或者点击数，如果一个用户有1个正样本，
10个负样本，另一个用户有5个正样本，50个负样本，这种差异同样需要消除。那么GAUC的计算，不仅将每个用户的AUC分开计算，
同时根据用户的展示数或者点击数来对每个用户的AUC进行加权处理。进一步消除了用户偏差对模型的影响。通过实验证明，
GAUC确实是一个更加合理的评价指标。</p>
</div>
<div class="section" id="id9">
<h3>模型原理<a class="headerlink" href="#id9" title="永久链接至标题">¶</a></h3>
<p><strong>base模型</strong></p>
<img alt="https://pic4.zhimg.com/80/v2-f9db847981d748788138ad653e9f0c1b_hd.jpg" src="https://pic4.zhimg.com/80/v2-f9db847981d748788138ad653e9f0c1b_hd.jpg" />
<p><strong>DIN</strong></p>
<img alt="https://pic1.zhimg.com/v2-57d865365f4eb0ba28006a2ae5d9e4dc_r.jpg" src="https://pic1.zhimg.com/v2-57d865365f4eb0ba28006a2ae5d9e4dc_r.jpg" />
<p>Base Model有一个很大的问题，它对用户的历史行为是同等对待的，没有做任何处理，这显然是不合理的。
一个很显然的例子，离现在越近的行为，越能反映你当前的兴趣。因此，对用户历史行为基于Attention机制进行一个加权</p>
<p>Attention机制简单的理解就是，针对不同的广告，用户历史行为与该广告的权重是不同的。假设用户有ABC三个历史行为，对于广告D，
那么ABC的权重可能是0.8、0.1、0.1；对于广告E，那么ABC的权重可能是0.3、0.6、0.1。这里的权重，
就是Attention机制即上图中的Activation Unit所需要学习的。</p>
<p>在加入Activation Unit之后，用户的兴趣表示计算如下：</p>
<blockquote>
<div><img alt="https://pic4.zhimg.com/80/v2-355981a18af3f6c1bd8eed721ab6d0e3_hd.jpg" src="https://pic4.zhimg.com/80/v2-355981a18af3f6c1bd8eed721ab6d0e3_hd.jpg" />
</div></blockquote>
<p>其中， <span class="math notranslate nohighlight">\({V}_{i}\)</span> 表示 <span class="math notranslate nohighlight">\(behavior id i\)</span> 的嵌入向量，比如good_id,shop_id等。
<span class="math notranslate nohighlight">\({V}_{u}\)</span> 是候选广告的嵌入向量；
<span class="math notranslate nohighlight">\({w}_{i}\)</span> 是候选广告影响着每个 <span class="math notranslate nohighlight">\(behavior id\)</span> 的权重，
也就是Local Activation。 <span class="math notranslate nohighlight">\({w}_{i}\)</span> 通过Activation Unit计算得出，这一块用函数去拟合，表示为 <span class="math notranslate nohighlight">\(g({V}_{i}, {V}_{a})\)</span> 。</p>
<p>Activation Unit 代码实现</p>
<p>这里的输入有三个，候选广告queries，用户历史行为keys，以及Batch中每个行为的长度。
这里为什么要输入一个keys_length呢，因为每个用户发生过的历史行为是不一样多的，
但是输入的keys维度是固定的(都是历史行为最大的长度)，因此我们需要这个长度来计算一个mask，
告诉模型哪些行为是没用的，哪些是用来计算用户兴趣分布的。</p>
<p>经过以下几个步骤得到用户的兴趣分布：</p>
<ul class="simple">
<li><ol class="arabic simple">
<li><p>将queries变为和keys同样的形状B * T * H(B指batch的大小，T指用户历史行为的最大长度，H指embedding的长度)</p></li>
</ol>
</li>
<li><ol class="arabic simple" start="2">
<li><p>通过三层神经网络得到queries和keys中每个key的权重，并经过softmax进行标准化</p></li>
</ol>
</li>
<li><ol class="arabic simple" start="3">
<li><p>通过weighted sum得到最终用户的历史行为分布</p></li>
</ol>
</li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">attention</span><span class="p">(</span><span class="n">queries</span><span class="p">,</span><span class="n">keys</span><span class="p">,</span><span class="n">keys_length</span><span class="p">):</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    queries:     [B, H]</span>
<span class="sd">    keys:        [B, T, H]</span>
<span class="sd">    keys_length: [B]</span>
<span class="sd">&#39;&#39;&#39;</span>

<span class="n">queries_hidden_units</span> <span class="o">=</span> <span class="n">queries</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">queries</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">queries</span><span class="p">,[</span><span class="mi">1</span><span class="p">,</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">keys</span><span class="p">)[</span><span class="mi">1</span><span class="p">]])</span>
<span class="n">queries</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">queries</span><span class="p">,[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">keys</span><span class="p">)[</span><span class="mi">1</span><span class="p">],</span><span class="n">queries_hidden_units</span><span class="p">])</span>

<span class="n">din_all</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">queries</span><span class="p">,</span><span class="n">keys</span><span class="p">,</span><span class="n">queries</span><span class="o">-</span><span class="n">keys</span><span class="p">,</span><span class="n">queries</span> <span class="o">*</span> <span class="n">keys</span><span class="p">],</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># B*T*4H</span>
<span class="c1"># 三层全链接</span>
<span class="n">d_layer_1_all</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">din_all</span><span class="p">,</span> <span class="mi">80</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;f1_att&#39;</span><span class="p">)</span>
<span class="n">d_layer_2_all</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">d_layer_1_all</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;f2_att&#39;</span><span class="p">)</span>
<span class="n">d_layer_3_all</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">d_layer_2_all</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;f3_att&#39;</span><span class="p">)</span> <span class="c1">#B*T*1</span>

<span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">d_layer_3_all</span><span class="p">,[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">keys</span><span class="p">)[</span><span class="mi">1</span><span class="p">]])</span> <span class="c1">#B*1*T</span>
<span class="c1"># Mask</span>
<span class="n">key_masks</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sequence_mask</span><span class="p">(</span><span class="n">keys_length</span><span class="p">,</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">keys</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">key_masks</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">key_masks</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># B*1*T</span>
<span class="n">paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="mi">2</span> <span class="o">**</span> <span class="mi">32</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># 在补足的地方附上一个很小的值，而不是0</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">key_masks</span><span class="p">,</span><span class="n">outputs</span><span class="p">,</span><span class="n">paddings</span><span class="p">)</span> <span class="c1"># B * 1 * T</span>
<span class="c1"># Scale</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span> <span class="o">/</span> <span class="p">(</span><span class="n">keys</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="c1"># Activation</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span> <span class="c1"># B * 1 * T</span>
<span class="c1"># Weighted Sum</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span><span class="n">keys</span><span class="p">)</span> <span class="c1"># B * 1 * H 三维矩阵相乘，相乘发生在后两维，即 B * (( 1 * T ) * ( T * H ))</span>
<span class="k">return</span> <span class="n">outputs</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">注解</p>
<dl>
<dt>softmax:</dt><dd><div class="math notranslate nohighlight" id="equation-data-mining-ctr-6">
<span class="eqno">()<a class="headerlink" href="#equation-data-mining-ctr-6" title="公式的永久链接">¶</a></span>\[\frac { { e }^{ { y }_{ i } } }{ \sum _{ j }^{ n }{ { e }^{ { y }_{ j } } }  }\]</div>
</dd>
<dt>logloss:</dt><dd><div class="math notranslate nohighlight" id="equation-data-mining-ctr-7">
<span class="eqno">()<a class="headerlink" href="#equation-data-mining-ctr-7" title="公式的永久链接">¶</a></span>\[-{ y }_{ i }\sum _{ i }^{ n }{ log\tilde { { y }_{ i } }  }\]</div>
</dd>
</dl>
</div>
</div>
<div class="section" id="id10">
<h3>激活函数<a class="headerlink" href="#id10" title="永久链接至标题">¶</a></h3>
</div>
<div class="section" id="id11">
<h3>自适应正则化<a class="headerlink" href="#id11" title="永久链接至标题">¶</a></h3>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>

  <hr/>


    <script src="https://utteranc.es/client.js"
            repo="zhangzhenhu/blog"
            issue-term="pathname"
            theme="github-light"
            crossorigin="anonymous"
            async>
    </script>



  <div role="contentinfo">
    <p>
        &#169; 版权所有 2018, zhangzhenhu(acmtiger@outlook.com) 禁止一切形式的转载！.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>.



</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>